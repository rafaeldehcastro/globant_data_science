<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[Career: IIS: RI: Improving Multi-Agent Reinforcement Learning for Cooperative, Partially Observable Settings]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2021</AwardEffectiveDate>
<AwardExpirationDate>02/28/2026</AwardExpirationDate>
<AwardTotalIntnAmount>549998.00</AwardTotalIntnAmount>
<AwardAmount>343320</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Roger Mailler</SignBlockName>
<PO_EMAI>rmailler@nsf.gov</PO_EMAI>
<PO_PHON>7032927982</PO_PHON>
</ProgramOfficer>
<AbstractNarration><![CDATA[As intelligent systems become more prevalent, these systems will need to coordinate with each other (e.g., apps, robots, autonomous cars), resulting in multi-agent systems. Allowing multi-agent systems to learn will let them operate in more complex and realistic scenarios by adapting their behavior to fit specific needs. Reinforcement learning is a promising form of trial-and-error learning that has the potential to drastically improve outcomes in many multi-agent domains (e.g., warehouses, delivery), but new methods are required for coordinating the agents in realistic domains with noisy and limited communication and sensing (i.e., partial observability). This project will develop these new reinforcement learning methods for coordinating teams of agents in various partially observable settings.  The results will impact the development of future artificial intelligence (AI) and robotic systems and will be conveyed through outreach and educational activities.&lt;br/&gt;&lt;br/&gt;This project will develop a number of novel methods for cooperative multi-agent reinforcement learning (MARL) under partial observability. MARL, the extension of reinforcement learning methods for multi-agent domains, has gained popularity for generating high-quality solutions in some domains, but more work is needed to make the methods more scalable and widely applicable. Therefore, this project will first provide a better theoretical understanding of centralized training for decentralized execution methods. Centralized training for decentralized execution is the dominant paradigm in MARL where agents are trained offline and only executed online. The project will then develop new centralized training methods that are unbiased, scalable and perform well in a wide range of domains.  Second, the project will develop online decentralized learning methods that allow agents to learn online even in noisy multi-agent settings. Lastly, to allow agents to learn and execute in an asynchronous manner, the project will develop methods for asynchronous MARL as well as asynchronous hierarchical learning with learning over multiple layers of a hierarchy. The resulting methods will significantly improve performance, stability and scalability of MARL methods and make them more generally applicable to large realistic domains.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.]]></AbstractNarration>
<MinAmdLetterDate>02/26/2021</MinAmdLetterDate>
<MaxAmdLetterDate>04/24/2023</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2044993</AwardID>
<Investigator>
<FirstName>Christopher</FirstName>
<LastName>Amato</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Christopher Amato</PI_FULL_NAME>
<EmailAddress><![CDATA[c.amato@northeastern.edu]]></EmailAddress>
<NSF_ID>000677651</NSF_ID>
<StartDate>02/26/2021</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Northeastern University</Name>
<CityName>BOSTON</CityName>
<ZipCode>021155005</ZipCode>
<PhoneNumber>6173733004</PhoneNumber>
<StreetAddress>360 HUNTINGTON AVE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>HLTMVS2JZBS6</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>NORTHEASTERN UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Northeastern University]]></Name>
<CityName>Boston</CityName>
<StateCode>MA</StateCode>
<ZipCode>021155005</ZipCode>
<StreetAddress><![CDATA[360 Huntington Ave.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code/>
<Name/>
<APP_SYMB_ID/>
</Appropriation>
<Appropriation>
<Code/>
<Name/>
<APP_SYMB_ID/>
</Appropriation>
<Appropriation>
<Code/>
<Name/>
<APP_SYMB_ID/>
</Appropriation>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002122DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<Fund>
<Code>01002223DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<Fund>
<Code>01002324DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<Fund>
<Code>01002122DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2021~122160</FUND_OBLG>
<FUND_OBLG>2022~109058</FUND_OBLG>
<FUND_OBLG>2023~112102</FUND_OBLG>
</Award>
</rootTag>
