<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[Statistical Estimation from Decoupled Data]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2020</AwardEffectiveDate>
<AwardExpirationDate>06/30/2023</AwardExpirationDate>
<AwardTotalIntnAmount>249999.00</AwardTotalIntnAmount>
<AwardAmount>249999</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Yong Zeng</SignBlockName>
<PO_EMAI>yzeng@nsf.gov</PO_EMAI>
<PO_PHON>7032927299</PO_PHON>
</ProgramOfficer>
<AbstractNarration><![CDATA[Modern statistics is defined by the fact that a great deal more data is available to practitioners than ever before. This is particularly the case in the sciences, where advances in experimental methodology across fields such as biology, chemistry, and physics have led to an explosion of different types of data, collected by different measurement apparatuses at potentially different times. Moreover, it may be difficult or impossible to connect data points from different experiments. For example, a chemist may apply two different measurement techniques to the same batch of molecules to obtain high-quality data about the whole batch, but it may be challenging to track the identities of particular molecules between measurements. The statistician who wishes to make the best possible inferences is faced with the difficult problem of how to integrate the data from different sources to conduct a unified analysis. Despite the ubiquity of this problem, rigorous statistical analyses of procedures designed to work with decoupled data are rare. The main goal of this project is to develop new tools for performing estimation tasks with decoupled data and to establish the fundamental limits of such techniques. This project will have impact on scientific and statistical methodology in both research and industrial settings.The graduate student support will be used on interdisciplinary research and writing codes.&lt;br/&gt; &lt;br/&gt;The project will investigate optimal rates of estimation for regression problems given access to decoupled data, and to establish potential trade-offs. Several intermediate regimes will be considered, for example, where the experimenter has access to many independent batches of shuffled data or to data with partial coupling information. This project will quantify the statistical price for learning with decoupled data via tight minimax bounds. This research is also aimed at establishing when minimax statistical procedures can be made computationally efficient, and investigating the possible presence of information theoretic-computational gaps in optimal rates of estimation.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.]]></AbstractNarration>
<MinAmdLetterDate>06/26/2020</MinAmdLetterDate>
<MaxAmdLetterDate>06/30/2022</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2015291</AwardID>
<Investigator>
<FirstName>Jonathan</FirstName>
<LastName>Niles-Weed</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jonathan Niles-Weed</PI_FULL_NAME>
<EmailAddress><![CDATA[jdw453@nyu.edu]]></EmailAddress>
<NSF_ID>000817263</NSF_ID>
<StartDate>06/26/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>New York University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100121019</ZipCode>
<PhoneNumber>2129982121</PhoneNumber>
<StreetAddress>70 WASHINGTON SQ S</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>NX9PXMKW5KW8</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>NEW YORK UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[New York University]]></Name>
<CityName>New York</CityName>
<StateCode>NY</StateCode>
<ZipCode>100121110</ZipCode>
<StreetAddress><![CDATA[251 Mercer Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0122</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<Fund>
<Code>01002122DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<Fund>
<Code>01002223DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~84987</FUND_OBLG>
<FUND_OBLG>2021~81429</FUND_OBLG>
<FUND_OBLG>2022~83583</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Many problems across the sciences---including physics, biology, and medicine---involve "uncoupled" data: measurements from the same population at different times or via different modalities but, crucially, without identifying information which would allow the individual members of the population from one time to be linked up with those from the second.<br />The goal of this project was to develop statistical and computational methods for analyzing such data. In addition, the proposal aimed at understanding the fundamental limits that govern such data analysis problems, and thereby to assess the optimality of the proposed methods.<br />Leveraging tools from probability theory, optimization, and geometry, this project succeeded at both aims. The research results revolved around several themes:1. Estimating transformations from uncoupled dataGiven a pair of uncoupled data sets collected from a population at different times, a core statistical problem is to estimate what aspects of the data changed. This change can be modeled as a function, which is unobserved, and whose properties need to be estimated solely on the basis of the "before" and "after" snapshots. This project developed three new procedures for solving this problem, corresponding to different settings and assumptions. These new procedures are also computationally efficient, meaning that they can be employed on large data sets involving tens of thousands of points in high dimensions. By contrast, all existing procedures either lacked theoretical guarantees or were only feasible for small data sets. Preliminary experiments on sytnn=hetic data are promising, and follow up research on the application of these methods in particle physics is currently underway.2. Asymptotic inferenceA reliable statistical method not only yields estimates of an unknown quantity, but also allows the inherent uncertainty in the estimate to be quantified. Towards that end, this project also developed new methodology for inference, which allows for valid confidence intervals to be constructed for unlabeled data. This portion of the project involved significant theoretical innovation, and gave rise to much stronger guarantees than had been available in prior work.3. Algorithms and HardnessA core goal of this project was to determine the computational limits of analysis of uncoupled data. Towards this goal, the project investigated several statistical inference problems involving uncoupled data that are computationally intractable. For each of these problems, this project shed new light on the reasons for their intractability. The research in this project also led to the development of new efficient algorithms.<br />These results were published in 10 refereed research articles and 4 publications currently under review.This project also supported the training of five graduate students.</p><br> <p>            Last Modified: 10/27/2023<br>      Modified by: Jonathan&nbsp;Niles-Weed</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Many problems across the sciences---including physics, biology, and medicine---involve "uncoupled" data: measurements from the same population at different times or via different modalities but, crucially, without identifying information which would allow the individual members of the population from one time to be linked up with those from the second. The goal of this project was to develop statistical and computational methods for analyzing such data. In addition, the proposal aimed at understanding the fundamental limits that govern such data analysis problems, and thereby to assess the optimality of the proposed methods. Leveraging tools from probability theory, optimization, and geometry, this project succeeded at both aims. The research results revolved around several themes:1. Estimating transformations from uncoupled dataGiven a pair of uncoupled data sets collected from a population at different times, a core statistical problem is to estimate what aspects of the data changed. This change can be modeled as a function, which is unobserved, and whose properties need to be estimated solely on the basis of the "before" and "after" snapshots. This project developed three new procedures for solving this problem, corresponding to different settings and assumptions. These new procedures are also computationally efficient, meaning that they can be employed on large data sets involving tens of thousands of points in high dimensions. By contrast, all existing procedures either lacked theoretical guarantees or were only feasible for small data sets. Preliminary experiments on sytnn=hetic data are promising, and follow up research on the application of these methods in particle physics is currently underway.2. Asymptotic inferenceA reliable statistical method not only yields estimates of an unknown quantity, but also allows the inherent uncertainty in the estimate to be quantified. Towards that end, this project also developed new methodology for inference, which allows for valid confidence intervals to be constructed for unlabeled data. This portion of the project involved significant theoretical innovation, and gave rise to much stronger guarantees than had been available in prior work.3. Algorithms and HardnessA core goal of this project was to determine the computational limits of analysis of uncoupled data. Towards this goal, the project investigated several statistical inference problems involving uncoupled data that are computationally intractable. For each of these problems, this project shed new light on the reasons for their intractability. The research in this project also led to the development of new efficient algorithms. These results were published in 10 refereed research articles and 4 publications currently under review.This project also supported the training of five graduate students.       Last Modified: 10/27/2023       Submitted by: Jonathan Niles-Weed]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
