<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[Collaborative Research: EAGER: SaTC-EDU: Safeguarding STEM Education and Scientific Knowledge in the Age of Hyper-Realistic Data Generated Using Artificial Intelligence]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2020</AwardEffectiveDate>
<AwardExpirationDate>08/31/2022</AwardExpirationDate>
<AwardTotalIntnAmount>99923.00</AwardTotalIntnAmount>
<AwardAmount>99923</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11010000</Code>
<Directorate>
<Abbreviation>EDU</Abbreviation>
<LongName>Directorate for STEM Education</LongName>
</Directorate>
<Division>
<Abbreviation>DGE</Abbreviation>
<LongName>Division Of Graduate Education</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ambareen Siraj</SignBlockName>
<PO_EMAI>asiraj@nsf.gov</PO_EMAI>
<PO_PHON>7032928182</PO_PHON>
</ProgramOfficer>
<AbstractNarration><![CDATA[The emergence of artificial intelligence (AI) systems that can create hyper-realistic data (e.g., images of human faces or network traffic data) presents challenges both to people and computers trying to determine what is authentic and what is fake. These advances pose both a threat and an opportunity for STEM learners and cybersecurity networks. On one hand, the ability of AI to generate hyper-realistic data has the potential to increase students’ interest in AI, STEM, and cybersecurity. On the other hand, AI-generated data, without robust cybersecurity guarantees, have the potential to reduce the veracity of knowledge that is publicly available on-line. This project proposes to conduct a series of studies where learners are presented with AI-generated STEM content and asked to determine its authenticity. The project seeks to discover whether differences exist in the level of vulnerabilities across diverse populations (K-12, higher education, and the adult workforce). The project will lay the foundation for a deeper understanding of the interconnectedness between STEM education materials and cybersecurity networks, and the commonalities that they face when challenged with the presence of hyper-realistic AI-generated data.   &lt;br/&gt;&lt;br/&gt;This NSF EAGER project brings together researchers from K-12 (Challenger Center), higher education (Carnegie Mellon University), and the workforce (RAND Corporation) to investigate risks posed to the free flow of STEM education materials and computer network traffic data in the age of hyper-realistic AI-generated data. Participants engaged in the study will be randomly shown fake STEM content (i.e., STEM content that is generated by Generative Neural Networks and has been modified to include misinformation) vs. STEM content that is authentic in its communication of STEM information . Each participant will be asked to classify whether the STEM content being displayed is fake or authentic. Additional questions will probe how specific characteristics of the STEM content displayed to participants serve as indicators of authenticity by randomly assigning participants versions of the STEM content that contain or omit those characteristics. The study of different learner populations (K-12, higher education, and the adult workforce) will elucidate the variability that exists amongst learners’ ability to decipher factual education material from AI-altered STEM education material, given the age and experience level of different learner populations. &lt;br/&gt;&lt;br/&gt;This project is supported by a special initiative of the Secure and Trustworthy Cyberspace (SaTC) program to foster new, previously unexplored, collaborations between the fields of cybersecurity, artificial intelligence, and education. The SaTC program aligns with the Federal Cybersecurity Research and Development Strategic Plan and the National Privacy Research Strategy to protect and preserve the growing social and economic benefits of cyber systems while ensuring security and privacy.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.]]></AbstractNarration>
<MinAmdLetterDate>07/27/2020</MinAmdLetterDate>
<MaxAmdLetterDate>07/27/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2039612</AwardID>
<Investigator>
<FirstName>Christopher</FirstName>
<LastName>Doss</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Christopher Doss</PI_FULL_NAME>
<EmailAddress><![CDATA[cdoss@rand.org]]></EmailAddress>
<NSF_ID>000764491</NSF_ID>
<StartDate>07/27/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jared</FirstName>
<LastName>Mondschein</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jared Mondschein</PI_FULL_NAME>
<EmailAddress><![CDATA[jmondsch@rand.org]]></EmailAddress>
<NSF_ID>000830731</NSF_ID>
<StartDate>07/27/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Rand Corporation</Name>
<CityName>SANTA MONICA</CityName>
<ZipCode>904013208</ZipCode>
<PhoneNumber>3103930411</PhoneNumber>
<StreetAddress>1776 MAIN ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>36</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA36</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>YY46Q97AEZA8</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>THE RAND CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[RAND Corporation]]></Name>
<CityName>Santa Monica</CityName>
<StateCode>CA</StateCode>
<ZipCode>904013297</ZipCode>
<StreetAddress><![CDATA[1776 Main Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>36</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA36</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8060</Code>
<Text>Secure &amp;Trustworthy Cyberspace</Text>
</ProgramElement>
<ProgramReference>
<Code>025Z</Code>
<Text>SaTC: Secure and Trustworthy Cyberspace</Text>
</ProgramReference>
<ProgramReference>
<Code>093Z</Code>
<Text>AI Education/Workforce Develop</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0420</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<Fund>
<Code>04002021DB</Code>
<Name><![CDATA[NSF Education & Human Resource]]></Name>
<FUND_SYMB_ID>040106</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~99923</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Science misinformation on topics ranging from climate change to vaccines have had significant public policy repercussions. While altered digital content has previously required significant expertise, artificial intelligence-based methods (e.g., deepfakes) threaten to lower the barriers to creating and disseminating highly realistic manipulated digital content. There is an increased risk of exposure to deepfakes among science education stakeholders as learners and educators increasingly rely on videos to obtain and share information, a trend accelerated by the COVID-19 pandemic. This project was the first study to understand the vulnerabilities of education stakeholders to science deepfakes and the characteristics of individuals and videos that moderate that vulnerability. The outcomes of the study throughout the life cycle are as follows:</p> <ol> <li>The project team took four videos of people espousing views on climate change and using current AI techniques manipulated the videos so that the subjects were espousing their opposite views (see Image 1 for an overview). Thus, climate change believers (two of the original videos) were manipulated to be climate change skeptics, and climate change skeptics (the other two original videos) were manipulated to be climate change believers. We refer to these manipulated videos as ?deepfakes.? In total, we produced a bank of 8 videos ? four non-manipulated videos (one for each of the speakers in which they communicate their authentic opinions) and four deepfakes (one for each of the speakers in which they are made to communicate their opposite opinion).</li> <li>The project team worked together to look at the prior literature on misinformation to understand the types of surveys that have been administered in the past. In addition, the study team collected validated scales that have been successfully deployed in the past. After synthesizing the literature, the study team created the survey that included:<ol> <li>The videos. The survey software randomly assigned whether they saw the authentic or deepfake video of each speaker. </li> <li>Questions to ascertain whether the respondent perceived a video to be authentic or manipulated and how confident they were about their perception</li> <li>Questions to ascertain which characteristics of the videos respondents analyzed to come to their conclusion</li> <li>Questions on the respondents? own beliefs on and knowledge of climate change, where respondents go to learn about climate change, how much they trust each source, internet use habits and access to the internet, perceptions on the threat of deepfakes, and background and demographic characteristics.</li> </ol></li> <li>The study team deployed a pilot survey and refined questions based on this pilot and feedback from academic partners.</li> <li>The study team deployed the refined survey to large samples of the five populations including: 805 middle school students across three states, a nationally representative sample of 740 principals, a nationally representative sample of 642 teachers, a nationally representative sample of 761 adults, and 87 undergraduate and graduate students from CMU.</li> <li>The study team created teacher materials to debrief the middle school students on the experience of taking the survey and the issues surrounding STEM deepfakes.</li> <li>The study team shared the results of the study to the 2021 American Society for Engineering Education annual conference, to an audience of more than 500 education and technology professionals at the 2021 Montgomery County Community College Technology &amp; Learning Conference, and to researcher and practitioners at the 2022 Association for Education Finance and Policy annual conference. A manuscript detailing the results of the full study is currently undergoing peer review.</li> <li>In August 2022, Challenger Center published a summary of our research findings and shared it with its nationwide audience of K-12 and post-secondary educators. We also intend to expand the study and pilot interventions to combat deepfake misinformation.</li> </ol> <p>We found that 33% to 50% of individuals are unable to distinguish between authentic videos and deepfakes (see Image 2). All populations exhibit vulnerability to deepfakes, but adults, including teachers and principals, exhibit higher vulnerability compared to students, indicating that those providing education, and thus the broader education system, are susceptible to deepfake-based science misinformation. Those who trust in information sources, whether in print or online, are more susceptible to deepfake-based information, suggesting that a healthy skepticism instilled by traditional media literacy curricula could be effective at combatting deepfake misinformation. Vulnerability, however, increases with more exposure to potential deepfakes, suggesting that deepfakes become more pernicious without interventions. Our results also suggest that education focused on the social context in which deepfakes reside, and not just the technical flaws of deepfake videos, is one promising strategy for combatting deepfake-based misinformation.</p><br> <p>            Last Modified: 12/21/2022<br>      Modified by: Christopher&nbsp;Doss</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2022/2039612/2039612_10689785_1671230284583_Image1_ExampleProcessofGeneratingFakeVideosforSurvey--rgov-214x142.jpg" original="/por/images/Reports/POR/2022/2039612/2039612_10689785_1671230284583_Image1_ExampleProcessofGeneratingFakeVideosforSurvey--rgov-800width.jpg" title="Example of Fake Video Generation Process"><img src="/por/images/Reports/POR/2022/2039612/2039612_10689785_1671230284583_Image1_ExampleProcessofGeneratingFakeVideosforSurvey--rgov-66x44.jpg" alt="Example of Fake Video Generation Process"></a> <div class="imageCaptionContainer"> <div class="imageCaption">This image depicts the process for creating the deepfake videos that we included in the survey.</div> <div class="imageCredit">Carnegie Mellon University</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Christopher&nbsp;Doss</div> <div class="imageTitle">Example of Fake Video Generation Process</div> </div> </li> <li> <a href="/por/images/Reports/POR/2022/2039612/2039612_10689785_1671230378087_Image2_PercentofResponsesCorrectlyIdentifyingtheAuthenticityofVideos--rgov-214x142.jpg" original="/por/images/Reports/POR/2022/2039612/2039612_10689785_1671230378087_Image2_PercentofResponsesCorrectlyIdentifyingtheAuthenticityofVideos--rgov-800width.jpg" title="Figure of Responses Correctly Identifying the Authenticity of Videos"><img src="/por/images/Reports/POR/2022/2039612/2039612_10689785_1671230378087_Image2_PercentofResponsesCorrectlyIdentifyingtheAuthenticityofVideos--rgov-66x44.jpg" alt="Figure of Responses Correctly Identifying the Authenticity of Videos"></a> <div class="imageCaptionContainer"> <div class="imageCaption">This image depicts survey results pertaining to the percent of responses correctly identifying the authenticity of videos, by video authenticity and population.</div> <div class="imageCredit">RAND Corporation</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Christopher&nbsp;Doss</div> <div class="imageTitle">Figure of Responses Correctly Identifying the Authenticity of Videos</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Science misinformation on topics ranging from climate change to vaccines have had significant public policy repercussions. While altered digital content has previously required significant expertise, artificial intelligence-based methods (e.g., deepfakes) threaten to lower the barriers to creating and disseminating highly realistic manipulated digital content. There is an increased risk of exposure to deepfakes among science education stakeholders as learners and educators increasingly rely on videos to obtain and share information, a trend accelerated by the COVID-19 pandemic. This project was the first study to understand the vulnerabilities of education stakeholders to science deepfakes and the characteristics of individuals and videos that moderate that vulnerability. The outcomes of the study throughout the life cycle are as follows:  The project team took four videos of people espousing views on climate change and using current AI techniques manipulated the videos so that the subjects were espousing their opposite views (see Image 1 for an overview). Thus, climate change believers (two of the original videos) were manipulated to be climate change skeptics, and climate change skeptics (the other two original videos) were manipulated to be climate change believers. We refer to these manipulated videos as ?deepfakes.? In total, we produced a bank of 8 videos ? four non-manipulated videos (one for each of the speakers in which they communicate their authentic opinions) and four deepfakes (one for each of the speakers in which they are made to communicate their opposite opinion). The project team worked together to look at the prior literature on misinformation to understand the types of surveys that have been administered in the past. In addition, the study team collected validated scales that have been successfully deployed in the past. After synthesizing the literature, the study team created the survey that included: The videos. The survey software randomly assigned whether they saw the authentic or deepfake video of each speaker.  Questions to ascertain whether the respondent perceived a video to be authentic or manipulated and how confident they were about their perception Questions to ascertain which characteristics of the videos respondents analyzed to come to their conclusion Questions on the respondents? own beliefs on and knowledge of climate change, where respondents go to learn about climate change, how much they trust each source, internet use habits and access to the internet, perceptions on the threat of deepfakes, and background and demographic characteristics.  The study team deployed a pilot survey and refined questions based on this pilot and feedback from academic partners. The study team deployed the refined survey to large samples of the five populations including: 805 middle school students across three states, a nationally representative sample of 740 principals, a nationally representative sample of 642 teachers, a nationally representative sample of 761 adults, and 87 undergraduate and graduate students from CMU. The study team created teacher materials to debrief the middle school students on the experience of taking the survey and the issues surrounding STEM deepfakes. The study team shared the results of the study to the 2021 American Society for Engineering Education annual conference, to an audience of more than 500 education and technology professionals at the 2021 Montgomery County Community College Technology &amp; Learning Conference, and to researcher and practitioners at the 2022 Association for Education Finance and Policy annual conference. A manuscript detailing the results of the full study is currently undergoing peer review. In August 2022, Challenger Center published a summary of our research findings and shared it with its nationwide audience of K-12 and post-secondary educators. We also intend to expand the study and pilot interventions to combat deepfake misinformation.   We found that 33% to 50% of individuals are unable to distinguish between authentic videos and deepfakes (see Image 2). All populations exhibit vulnerability to deepfakes, but adults, including teachers and principals, exhibit higher vulnerability compared to students, indicating that those providing education, and thus the broader education system, are susceptible to deepfake-based science misinformation. Those who trust in information sources, whether in print or online, are more susceptible to deepfake-based information, suggesting that a healthy skepticism instilled by traditional media literacy curricula could be effective at combatting deepfake misinformation. Vulnerability, however, increases with more exposure to potential deepfakes, suggesting that deepfakes become more pernicious without interventions. Our results also suggest that education focused on the social context in which deepfakes reside, and not just the technical flaws of deepfake videos, is one promising strategy for combatting deepfake-based misinformation.       Last Modified: 12/21/2022       Submitted by: Christopher Doss]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
