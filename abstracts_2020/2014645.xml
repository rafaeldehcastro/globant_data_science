<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>STTR Phase I:  Biomimetic Solution for Gesture-Based Human Machine Interactions</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2020</AwardEffectiveDate>
<AwardExpirationDate>08/31/2021</AwardExpirationDate>
<AwardTotalIntnAmount>224999.00</AwardTotalIntnAmount>
<AwardAmount>244999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>15030000</Code>
<Directorate>
<Abbreviation>TIP</Abbreviation>
<LongName>Dir for Tech, Innovation, &amp; Partnerships</LongName>
</Directorate>
<Division>
<Abbreviation>TI</Abbreviation>
<LongName>Translational Impacts</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Diane Hickey</SignBlockName>
<PO_EMAI>dhickey@nsf.gov</PO_EMAI>
<PO_PHON>7032928875</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The broader/commercial opportunity of this Small Business Technology Transfer (STTR) Phase I project is to address the emerging demand for intuitive human-machine interfaces. Interest in virtual and augmented reality (VR/AR) technologies has increased substantially, but current VR/AR interface algorithms are often inefficient and hamper many gesture-based interfaces, due to limitations in current machine learning algorithms. This limits the performance and widespread adoption of new VR/AR technologies. The technology developed here will enable intuitive interactions with VR/AR that will greatly expand the scope of commercial applications and shorten the development timelines for new products.  &lt;br/&gt;&lt;br/&gt;This Small Business Technology Transfer (STTR) Phase I project will develop a prototype musculoskeletal technology for decoding movement from muscle activity. Traditional machine-learning-based gesture recognition approaches for virtual and augmented reality (VR/AR) interfaces cannot fulfill the demand for more natural and intuitive interactions, due to the reliance on only motion observation.  Current gesture recognition technologies are often not generalizable to novel or perturbed movements. The goal of this project is to develop a backend technology for real-time and accurate decoding of intended arm and hand movement from muscle activity, which overcomes both of those limitations. This project will lead to a new tool for continuous real-time computer interface with multidimensional hand gestures or movements.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>05/22/2020</MinAmdLetterDate>
<MaxAmdLetterDate>12/18/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2014645</AwardID>
<Investigator>
<FirstName>Trevor</FirstName>
<LastName>Moon</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Trevor R Moon</PI_FULL_NAME>
<EmailAddress>trevor@neuro-wired.com</EmailAddress>
<PI_PHON>3046578673</PI_PHON>
<NSF_ID>000825812</NSF_ID>
<StartDate>05/22/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Amelia</FirstName>
<LastName>Adcock</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Amelia Adcock</PI_FULL_NAME>
<EmailAddress>akadcock@hsc.wvu.edu</EmailAddress>
<PI_PHON>3045986127</PI_PHON>
<NSF_ID>000825838</NSF_ID>
<StartDate>05/22/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>NEUROWIRED</Name>
<CityName>MORGANTOWN</CityName>
<ZipCode>265012414</ZipCode>
<PhoneNumber>3046022026</PhoneNumber>
<StreetAddress>60 WESTMINISTER DR</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>West Virginia</StateName>
<StateCode>WV</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WV01</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>LJAEGD8UUMN3</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>NEUROWIRED</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[West Virginia University Research Corporation]]></Name>
<CityName>Morgantown</CityName>
<StateCode>WV</StateCode>
<ZipCode>265066845</ZipCode>
<StreetAddress><![CDATA[886 Chestnut Ridge Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>West Virginia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WV01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1505</Code>
<Text>STTR Phase I</Text>
</ProgramElement>
<ProgramElement>
<Code>8091</Code>
<Text>SBIR Outreach &amp; Tech. Assist</Text>
</ProgramElement>
<ProgramReference>
<Code>079E</Code>
<Text>VISUALIZATION &amp; VIRTUAL DESIGN</Text>
</ProgramReference>
<ProgramReference>
<Code>8033</Code>
<Text>Hardware Software Integration</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<Fund>
<Code>01002122DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~224999</FUND_OBLG>
<FUND_OBLG>2021~20000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Neurowired, LLC aims to decode a person's intended movement(s) and transform it into desired output(s) (e.g., contactless swipe control) for robust interactions between humans and technology. Neurowired, a software company, develops computer algorithms for existing and future hardware-software applications to provide a platform for human-machine interfaces (HMIs). Our mission is to achieve robust and intuitive solutions inspired by biological functions. In contrast to alternative solutions, our technology is a real-time (less than 2-millisecond delays) software to control devices from muscle activity. Using biomechanical transformations to continuously predict intended movement, this technology can disrupt multiple markets&nbsp; by introducing a new level of control and user experience never seen before. For example, it can be used in augmented and virtual reality (AR/VR) for entertainment and rehabilitation, tracking metrics in smart wearables (e.g., Apple Watch), human performance training and assessment programs for athletes, educators, and high-skill professionals, and many other fields.</p> <p>The goal of our NSF STTR Phase I Award was to develop a prototype of the biomimetic algorithms, i.e., the biomechanical transformation from muscle activity to muscle forces. Traditional artificial intelligence/machine learning (AI/ML) approaches that use gesture recognition (i.e., classifiers) trained on large datasets cannot satisfy the large, possible range of movements or postures needed for natural, intuitive human-machine interfaces; moreover, the time required for AI/ML training is not suitable for plug-and-play applications. Here, we developed a platform for an accurate, real-time transformation of muscle activity to an intended movement that does not require training, overcoming the limitations of the alternative solutions. Furthermore, our approach reduces the need for large and costly development time to fine-tune an algorithm and decreases the time-to-market and barrier-to-entry. The NSF I-Corps programs guided us through performing extensive market research to focus our development on existing problems to deliver needed solutions.</p> <p>Throughout the award, we held numerous successful conversations with users, decision-makers, and influencers in the spaces of prosthetics, AR/VR, and medical education, as well as potential strategic partners. As a result, we pivoted to providing data-driven solutions for medical professions involved in evaluating and training robotic surgery. Currently, surgeons still rely on the same case observation and qualitative assessments that have been around for dozens of years. This approach is difficult to standardize without objective data due to the variability in the quality of training and assessment (e.g., mentors, institutions). So, we conducted a study (Gritsenko, et al., 2021. IEEE Xplore) with West Virginia University Surgical Oncologists to track skill progression with objective metrics like muscle activity in novice medical residents to determine if quantitative data could complement/replace current training and evaluation. Our results demonstrated that muscle activity can capture the process of learning surgical skills, and more interestingly, indicated different learning rates across objective and current metrics, e.g., time of completion. These advancements, along with our continued development of our models and computer algorithms, are the basis for an NSF STTR Phase 2, which aims to develop quantitative tools for training and assessment of surgical skills in robotic surgery.</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/30/2021<br>      Modified by: Trevor&nbsp;R&nbsp;Moon</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Neurowired, LLC aims to decode a person's intended movement(s) and transform it into desired output(s) (e.g., contactless swipe control) for robust interactions between humans and technology. Neurowired, a software company, develops computer algorithms for existing and future hardware-software applications to provide a platform for human-machine interfaces (HMIs). Our mission is to achieve robust and intuitive solutions inspired by biological functions. In contrast to alternative solutions, our technology is a real-time (less than 2-millisecond delays) software to control devices from muscle activity. Using biomechanical transformations to continuously predict intended movement, this technology can disrupt multiple markets  by introducing a new level of control and user experience never seen before. For example, it can be used in augmented and virtual reality (AR/VR) for entertainment and rehabilitation, tracking metrics in smart wearables (e.g., Apple Watch), human performance training and assessment programs for athletes, educators, and high-skill professionals, and many other fields.  The goal of our NSF STTR Phase I Award was to develop a prototype of the biomimetic algorithms, i.e., the biomechanical transformation from muscle activity to muscle forces. Traditional artificial intelligence/machine learning (AI/ML) approaches that use gesture recognition (i.e., classifiers) trained on large datasets cannot satisfy the large, possible range of movements or postures needed for natural, intuitive human-machine interfaces; moreover, the time required for AI/ML training is not suitable for plug-and-play applications. Here, we developed a platform for an accurate, real-time transformation of muscle activity to an intended movement that does not require training, overcoming the limitations of the alternative solutions. Furthermore, our approach reduces the need for large and costly development time to fine-tune an algorithm and decreases the time-to-market and barrier-to-entry. The NSF I-Corps programs guided us through performing extensive market research to focus our development on existing problems to deliver needed solutions.  Throughout the award, we held numerous successful conversations with users, decision-makers, and influencers in the spaces of prosthetics, AR/VR, and medical education, as well as potential strategic partners. As a result, we pivoted to providing data-driven solutions for medical professions involved in evaluating and training robotic surgery. Currently, surgeons still rely on the same case observation and qualitative assessments that have been around for dozens of years. This approach is difficult to standardize without objective data due to the variability in the quality of training and assessment (e.g., mentors, institutions). So, we conducted a study (Gritsenko, et al., 2021. IEEE Xplore) with West Virginia University Surgical Oncologists to track skill progression with objective metrics like muscle activity in novice medical residents to determine if quantitative data could complement/replace current training and evaluation. Our results demonstrated that muscle activity can capture the process of learning surgical skills, and more interestingly, indicated different learning rates across objective and current metrics, e.g., time of completion. These advancements, along with our continued development of our models and computer algorithms, are the basis for an NSF STTR Phase 2, which aims to develop quantitative tools for training and assessment of surgical skills in robotic surgery.          Last Modified: 11/30/2021       Submitted by: Trevor R Moon]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
