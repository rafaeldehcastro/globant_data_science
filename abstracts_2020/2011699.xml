<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[Deep Learning for Inverse Problems]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2020</AwardEffectiveDate>
<AwardExpirationDate>08/31/2024</AwardExpirationDate>
<AwardTotalIntnAmount>400000.00</AwardTotalIntnAmount>
<AwardAmount>400000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Yuliya Gorb</SignBlockName>
<PO_EMAI>ygorb@nsf.gov</PO_EMAI>
<PO_PHON>7032922113</PO_PHON>
</ProgramOfficer>
<AbstractNarration><![CDATA[In the past few years, deep learning has become the dominant approach for computer vision, image processing, speech recognition, and many other applications in machine learning and data science. This success is a synergy of several ingredients: (1) neural networks (NNs) as a flexible framework for representing high-dimensional functions and maps, (2) simple algorithms such as back-propagation and stochastic gradient descent for tuning the model parameters, (3) highly effective general software packages such as Tensorflow and Pytorch, and (4) unprecedented computing power. Despite the successes however, several key challenges remain: (1) the NN architectural design is still an art and it lacks basic mathematical principles in many cases, and (2) the NN training often requires an enormous amount of data, which is not available in many applications. Many computational problems in physical sciences also face the same challenges as those in data science: high-dimensionality, complicated or unspecified models, and large computational cost. Some well-known examples are many-body quantum systems, deterministic and stochastic control, molecular dynamics, uncertainty quantification, and inverse problems. It is natural to leverage the recent developments of NNs in the study of these problems. This project focuses on inverse problems, that is, recovering unknown problem parameters from observation data. It is a field of enormous importance, with applications in physics, chemistry, medicine, earth sciences, and defense. However, many inverse problems are known to be computationally challenging. This project will use deep learning as a means for solving these inverse problems more efficiently. The PI plans to develop a new applied linear algebra course with a machine learning focus and to organize workshops at the interface of deep learning and physical sciences. The project will train graduate and undergraduate students through the research and support one graduate student per year.&lt;br/&gt;&lt;br/&gt;As a tool for representing high-dimensional maps, neural nets (NN) offer a flexible way for representing the full inverse maps and learn the data distribution prior via training. The rich mathematical and physical theories behind inverse problems provide theoretical guidance for designing compact yet effective NN architectures and hence it is possible to avoid the need for an enormous amount of data. In the theoretical part, this project plans to identify the commonly-used mathematical operators in many inverse problems and design novel NN modules for these operators. Two such examples are the pseudodifferential operators and the Fourier integral operators. Using analytical results from partial differential equations and numerical linear algebra, both types of operators can be represented compactly and accurately as NN modules. In the application part, this project considers five inverse problems: electric impedance tomography, optical tomography, inverse acoustic/electromagnetic scattering, seismic imaging, and travel-time tomography. The NN for the inverse map is then assembled using the modules from the theory part, along with existing primitives such as convolutional NN. The whole NN is then trained end-to-end with the training data. The research results such as publications and software will be made available in public domain. The project will also lead to broader impacts in several areas. First, this project advocates for a more principled approach for NN architecture design based on mathematical theories, sparsity considerations, and invariance/equivariance principles. Second, the project covers inverse problems from five different topics. By working with scientists and engineers, the PI plans to apply the technologies developed to practical applications.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.]]></AbstractNarration>
<MinAmdLetterDate>06/26/2020</MinAmdLetterDate>
<MaxAmdLetterDate>06/26/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2011699</AwardID>
<Investigator>
<FirstName>Lexing</FirstName>
<LastName>Ying</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lexing Ying</PI_FULL_NAME>
<EmailAddress><![CDATA[lexing@math.stanford.edu]]></EmailAddress>
<NSF_ID>000148894</NSF_ID>
<StartDate>06/26/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Stanford University</Name>
<CityName>STANFORD</CityName>
<ZipCode>943052004</ZipCode>
<PhoneNumber>6507232300</PhoneNumber>
<StreetAddress>450 JANE STANFORD WAY</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>16</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA16</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>HJD6G4D6TJY5</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>THE LELAND STANFORD JUNIOR UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Stanford University]]></Name>
<CityName>Stanford</CityName>
<StateCode>CA</StateCode>
<ZipCode>943052125</ZipCode>
<StreetAddress><![CDATA[450 Jane Stanford Way]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>16</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA16</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1271</Code>
<Text>COMPUTATIONAL MATHEMATICS</Text>
</ProgramElement>
<ProgramReference>
<Code>075Z</Code>
<Text>Artificial Intelligence (AI)</Text>
</ProgramReference>
<ProgramReference>
<Code>079Z</Code>
<Text>Machine Learning Theory</Text>
</ProgramReference>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~400000</FUND_OBLG>
</Award>
</rootTag>
