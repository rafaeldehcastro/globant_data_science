<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[CC* Compute: Accelerating Science and Education by Campus and Grid Computing]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2020</AwardEffectiveDate>
<AwardExpirationDate>06/30/2023</AwardExpirationDate>
<AwardTotalIntnAmount>399938.00</AwardTotalIntnAmount>
<AwardAmount>399938</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kevin Thompson</SignBlockName>
<PO_EMAI>kthompso@nsf.gov</PO_EMAI>
<PO_PHON>7032924220</PO_PHON>
</ProgramOfficer>
<AbstractNarration><![CDATA[High-performance computing resources are a fundamental need of modern research that unites almost all disciplines. Experience with these resources is also an important tool for preparing today's students for a wide range of careers. A group of researchers and educators at the University of Colorado Denver, in partnership with its Office of Information Technology, are building a state-of-the-art computing resource on its Downtown Campus. The new facility provides the first campus-wide high-performance computer system to support both research and teaching efforts. The computing cluster is integrated with the Open Science Grid (OSG), enabling access to additional computing resources from partner institutions, and sharing unused time with the wider community. A high-priority educational queue is dedicated for teaching and course-based research. &lt;br/&gt;&lt;br/&gt;The resource will include 2048 AMD EPYC compute cores and 16TB memory distributed across 32 compute nodes; 2 high-memory nodes, each with 2TB memory and 64 cores; one NVIDIA Tesla V100 32GB GPU; 1PB (raw) storage; and InfiniBand interconnect. For data-intensive research and access of OSG jobs to distributed data, the cluster is configured with full end-to-end 10gb/s connectivity from each node to Internet 2. A graphical Jupyter notebook interface increases accessibility. &lt;br/&gt;&lt;br/&gt;The configuration addresses computing requirements based on a survey and an analysis of the needs of science and educational drivers in fields including earth and environmental sciences, biotechnology and genomics, computer science and engineering, applied mathematics, physics, and business. The resource will broaden participation in computational science and have a significant impact on the supported research.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.]]></AbstractNarration>
<MinAmdLetterDate>06/16/2020</MinAmdLetterDate>
<MaxAmdLetterDate>06/16/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2019089</AwardID>
<Investigator>
<FirstName>Jan</FirstName>
<LastName>Mandel</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jan Mandel</PI_FULL_NAME>
<EmailAddress><![CDATA[Jan.Mandel@ucdenver.edu]]></EmailAddress>
<NSF_ID>000419389</NSF_ID>
<StartDate>06/16/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kannan</FirstName>
<LastName>Premnath</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kannan Premnath</PI_FULL_NAME>
<EmailAddress><![CDATA[kannan.premnath@ucdenver.edu]]></EmailAddress>
<NSF_ID>000655868</NSF_ID>
<StartDate>06/16/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Carlos</FirstName>
<LastName>Infante</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Carlos R Infante</PI_FULL_NAME>
<EmailAddress><![CDATA[Carlos.Infante@UCDenver.edu]]></EmailAddress>
<NSF_ID>000690468</NSF_ID>
<StartDate>06/16/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Amy</FirstName>
<LastName>Roberts</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Amy L Roberts</PI_FULL_NAME>
<EmailAddress><![CDATA[amy.roberts@ucdenver.edu]]></EmailAddress>
<NSF_ID>000758861</NSF_ID>
<StartDate>06/16/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Yaning</FirstName>
<LastName>Liu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yaning Liu</PI_FULL_NAME>
<EmailAddress><![CDATA[yaning.liu@ucdenver.edu]]></EmailAddress>
<NSF_ID>000819877</NSF_ID>
<StartDate>06/16/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Colorado at Denver-Downtown Campus</Name>
<CityName>DENVER</CityName>
<ZipCode>802042055</ZipCode>
<PhoneNumber>3037240090</PhoneNumber>
<StreetAddress>1380 LAWRENCE ST STE 300</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<StateCode>CO</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CO01</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>M6CXZ6GSJW84</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF COLORADO AT DENVER</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Colorado at Denver-Downtown Campus]]></Name>
<CityName>Denver</CityName>
<StateCode>CO</StateCode>
<ZipCode>802173364</ZipCode>
<StreetAddress><![CDATA[1201 Larimer Street, Suite 4000]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CO01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8080</Code>
<Text>Campus Cyberinfrastructure</Text>
</ProgramElement>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~399938</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p style="box-sizing: inherit; margin: 0px 0px 8px; padding: 0px; caret-color: #1b1b1b; color: #1b1b1b; font-family: Verdana; font-size: 11.311999px; font-style: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: auto; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; text-decoration: none;">This CC* grant supported the purchase of a 2,048 core cluster with InfiniBand networking, based on AMD EPYC2 processors (32 nodes with 64 cores and 512GB memory each), plus 2 high-memory/GPU nodes with 2TB memory and NVIDIA A-100 GPU each. &nbsp;We were later able to add two more&nbsp;<span>A-100 GPUs. The cluster has a dedicated 816 TB (1PB raw) disk array on the InfiniBand network for project and scratch space. It is integrated with two other clusters by sharing a Slurm scheduler and user home directories on the network.</span></p> <p style="box-sizing: inherit; margin: 0px 0px 8px; padding: 0px; caret-color: #1b1b1b; color: #1b1b1b; font-family: Verdana; font-size: 11.311999px; font-style: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: auto; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; text-decoration: none;"><span>The cluster is providing 20% cycles to OSG.</span></p> <p style="box-sizing: inherit; margin: 0px 0px 8px; padding: 0px; caret-color: #1b1b1b; color: #1b1b1b; font-family: Verdana; font-size: 11.311999px; font-style: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: auto; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; text-decoration: none;"><span style="font-size: 11.311999px;">The cluster has supported a significant number of science and engineering projects, publications, theses, and presentations, including student presentations, in fields including genomics, bioengineering, fluid mechanics, optimization for equity, machine learning and AI, coupled atmosphere-fire simulation, and discrete mathematics. It has exposed a generation of students to cluster computing and allowed them to build their skills. A number of funded projects now relies on the facility.&nbsp;</span></p> <p style="box-sizing: inherit; margin: 0px 0px 8px; padding: 0px; caret-color: #1b1b1b; color: #1b1b1b; font-family: Verdana; font-size: 11.311999px; font-style: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: auto; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; text-decoration: none;"><span style="font-size: 11.311999px;">Unlike large off-site<span>&nbsp;</span>shared supercomputing facilities, the cluster has been able to provide generally little or no queue wait time, while keeping cluster utilization high. The fast turnaround and the availability of large persistent storage had a transformative impact on research on campus, particularly student research.&nbsp;</span></p> <p style="box-sizing: inherit; margin: 0px 0px 8px; padding: 0px; caret-color: #1b1b1b; color: #1b1b1b; font-family: Verdana; font-size: 11.311999px; font-style: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: auto; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; text-decoration: none;"><span style="font-size: 11.311999px;">The cluster has expected useful life at least 5 years, i.e., until 2026.</span></p> <p style="box-sizing: inherit; margin: 0px 0px 8px; padding: 0px; caret-color: #1b1b1b; color: #1b1b1b; font-family: Verdana; font-size: 11.311999px; font-style: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: auto; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; text-decoration: none;">&nbsp;</p><br> <p>            Last Modified: 10/27/2023<br>      Modified by: Jan&nbsp;Mandel</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[This CC* grant supported the purchase of a 2,048 core cluster with InfiniBand networking, based on AMD EPYC2 processors (32 nodes with 64 cores and 512GB memory each), plus 2 high-memory/GPU nodes with 2TB memory and NVIDIA A-100 GPU each.  We were later able to add two more A-100 GPUs. The cluster has a dedicated 816 TB (1PB raw) disk array on the InfiniBand network for project and scratch space. It is integrated with two other clusters by sharing a Slurm scheduler and user home directories on the network. The cluster is providing 20% cycles to OSG. The cluster has supported a significant number of science and engineering projects, publications, theses, and presentations, including student presentations, in fields including genomics, bioengineering, fluid mechanics, optimization for equity, machine learning and AI, coupled atmosphere-fire simulation, and discrete mathematics. It has exposed a generation of students to cluster computing and allowed them to build their skills. A number of funded projects now relies on the facility.  Unlike large off-site shared supercomputing facilities, the cluster has been able to provide generally little or no queue wait time, while keeping cluster utilization high. The fast turnaround and the availability of large persistent storage had a transformative impact on research on campus, particularly student research.  The cluster has expected useful life at least 5 years, i.e., until 2026.         Last Modified: 10/27/2023       Submitted by: Jan Mandel]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
