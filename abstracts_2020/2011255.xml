<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[CAREER: Efficient Algorithms for Learning and Testing Structured Probabilistic Models]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/11/2019</AwardEffectiveDate>
<AwardExpirationDate>01/31/2023</AwardExpirationDate>
<AwardTotalIntnAmount>464337.00</AwardTotalIntnAmount>
<AwardAmount>464337</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>A. Funda Ergun</SignBlockName>
<PO_EMAI>fergun@nsf.gov</PO_EMAI>
<PO_PHON>7032922216</PO_PHON>
</ProgramOfficer>
<AbstractNarration><![CDATA[In recent years, the amount of available data in science and technology has exploded and is currently expanding at an unprecedented rate. The general task of making accurate inferences on large and complex datasets has become a major bottleneck across various disciplines. A natural formalization of such inference tasks involves  viewing the data as random samples drawn from a probabilistic model -- a model that we believe describes the process generating the data. The overarching goal of this project is to obtain a refined understanding of these inference tasks from both statistical and computational perspectives. The questions addressed in this project arise from pressing challenges faced in modern data analysis. A crucial component of the project involves fostering collaboration between different communities. Furthermore, the PI will mentor high-school and undergraduate students, and design several new theory courses integrating research and teaching at the undergraduate and graduate levels.&lt;br/&gt;&lt;br/&gt;The PI will investigate several fundamental algorithmic questions in unsupervised learning and testing for which there is an alarming gap in our current understanding. These include designing efficient algorithms that are stable in the presence of deviations from the assumed model, circumventing the curse of dimensionality in distribution learning, and testing high-dimensional probabilistic models. This set of directions could lead to new algorithmic and probabilistic techniques, and offer insights into the interplay between structure and efficiency in unsupervised estimation. This research ties into a broader range of work across computer science, probability, statistics, and information theory.]]></AbstractNarration>
<MinAmdLetterDate>02/07/2020</MinAmdLetterDate>
<MaxAmdLetterDate>08/04/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2011255</AwardID>
<Investigator>
<FirstName>Ilias</FirstName>
<LastName>Diakonikolas</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ilias Diakonikolas</PI_FULL_NAME>
<EmailAddress><![CDATA[iliasdiakonikolas@gmail.com]]></EmailAddress>
<NSF_ID>000723398</NSF_ID>
<StartDate>02/07/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Wisconsin-Madison</Name>
<CityName>MADISON</CityName>
<ZipCode>537151218</ZipCode>
<PhoneNumber>6082623822</PhoneNumber>
<StreetAddress>21 N PARK ST STE 6301</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Wisconsin</StateName>
<StateCode>WI</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WI02</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>LCLSJAGTNZQ7</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WISCONSIN SYSTEM</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM>QAFKGG7YY396</ORG_PRNT_UEI_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Wisconsin-Madison]]></Name>
<CityName>Madison</CityName>
<StateCode>WI</StateCode>
<ZipCode>537151218</ZipCode>
<StreetAddress><![CDATA[21 N Park Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Wisconsin</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WI02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>2878</Code>
<Text>Special Projects - CCF</Text>
</ProgramElement>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01001718DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<Fund>
<Code>01001920DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2017~131087</FUND_OBLG>
<FUND_OBLG>2019~108540</FUND_OBLG>
<FUND_OBLG>2020~224710</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project develops a general algorithmic theory of high-dimensional (outlier-) robust learning and statistics. The broad task of learning in the presence of corrupted data has been studied in the statistics community over several decades&nbsp;with a focus on statistical efficiency. On the other hand, until fairly recently,&nbsp;the computational aspects of this field were poorly understood.&nbsp;This project gives the first computationally efficient robust estimators in high dimensions&nbsp;for a range of learning tasks, including mean and covariance estimation,&nbsp;linear regression and classification, learning mixture models, stochastic optimization,&nbsp;and learning in the presence of sparsity. These efficient estimators have led to practical&nbsp;improvements in a number of pressing applications, including in the analysis of genetic data&nbsp;and in adversarial machine learning. The PI wrote a book on the topic that covers these&nbsp;recent developments and is published by Cambridge University Press.&nbsp;&nbsp;</p> <p>A second direction explored by this project is to design efficient learning and testing&nbsp;algorithms for broad classes of structured distributions. The project develops&nbsp;general statistical and algorithmic methodology that leads to the first sample and computationally efficient&nbsp;methods in a wide range of settings.&nbsp;</p> <p>Finally, the project supported the research training of several graduate students. Additionally,&nbsp;the PI supervised undergraduate research projects, as part of the WISCERS program at UW Madison.</p><br> <p>            Last Modified: 07/07/2023<br>      Modified by: Ilias&nbsp;Diakonikolas</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project develops a general algorithmic theory of high-dimensional (outlier-) robust learning and statistics. The broad task of learning in the presence of corrupted data has been studied in the statistics community over several decades with a focus on statistical efficiency. On the other hand, until fairly recently, the computational aspects of this field were poorly understood. This project gives the first computationally efficient robust estimators in high dimensions for a range of learning tasks, including mean and covariance estimation, linear regression and classification, learning mixture models, stochastic optimization, and learning in the presence of sparsity. These efficient estimators have led to practical improvements in a number of pressing applications, including in the analysis of genetic data and in adversarial machine learning. The PI wrote a book on the topic that covers these recent developments and is published by Cambridge University Press.    A second direction explored by this project is to design efficient learning and testing algorithms for broad classes of structured distributions. The project develops general statistical and algorithmic methodology that leads to the first sample and computationally efficient methods in a wide range of settings.   Finally, the project supported the research training of several graduate students. Additionally, the PI supervised undergraduate research projects, as part of the WISCERS program at UW Madison.       Last Modified: 07/07/2023       Submitted by: Ilias Diakonikolas]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
