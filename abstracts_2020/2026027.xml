<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[SBIR Phase II:  AI-Driven Orientation and Mobility System for the Blind]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>12/15/2020</AwardEffectiveDate>
<AwardExpirationDate>07/31/2023</AwardExpirationDate>
<AwardTotalIntnAmount>999998.00</AwardTotalIntnAmount>
<AwardAmount>999998</AwardAmount>
<AwardInstrument>
<Value>Cooperative Agreement</Value>
</AwardInstrument>
<Organization>
<Code>15030000</Code>
<Directorate>
<Abbreviation>TIP</Abbreviation>
<LongName>Dir for Tech, Innovation, &amp; Partnerships</LongName>
</Directorate>
<Division>
<Abbreviation>TI</Abbreviation>
<LongName>Translational Impacts</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Alastair Monk</SignBlockName>
<PO_EMAI>amonk@nsf.gov</PO_EMAI>
<PO_PHON>7032924392</PO_PHON>
</ProgramOfficer>
<AbstractNarration><![CDATA[The broader impact/commercial potential of this Small Business Innovation Research Phase II project is to advance computer vision for navigation to support independence for individuals with visual impairment. Roughly eight 8 million individuals in the US live with blindness or visual impairment, which severely reduces a personâ€™s ability to independently navigate in new spaces and consequently limits participation in social and economic life. The lack of independence directly leads to disadvantages in completing formal education and joining the workforce. This project is aimed at creating a breakthrough orientation and mobility system as an application narrating environments and providing the ability to record and retrace routes in indoor environments. The core technological innovation is a multimodal neural network architecture capable of learning robust spatial representations and efficiently operating on mobile devices. &lt;br/&gt;  &lt;br/&gt;The proposed project will introduce a novel artificial intelligence (AI)-driven Orientation and Mobility System to assist individuals with visual impairment in crucial navigation tasks. This proposal will generate contextually relevant, task-oriented spatial information from smartphone cameras. The research tasks include: (1) Development of optimization and data augmentation techniques for improved runtime performance for a broad range of mobile devices; (2) Development of verbal narration of environments as well as route creation and tracking, (3) Co-design and testing with potential users.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.]]></AbstractNarration>
<MinAmdLetterDate>12/16/2020</MinAmdLetterDate>
<MaxAmdLetterDate>12/16/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>CoopAgrmnt</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2026027</AwardID>
<Investigator>
<FirstName>Cagri</FirstName>
<LastName>Zaman</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Cagri H Zaman</PI_FULL_NAME>
<EmailAddress><![CDATA[cagri@mediate.tech]]></EmailAddress>
<NSF_ID>000782918</NSF_ID>
<StartDate>12/16/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>VIRTUAL COLLABORATION RESEARCH INC.</Name>
<CityName>SOMERVILLE</CityName>
<ZipCode>021433608</ZipCode>
<PhoneNumber>3144458028</PhoneNumber>
<StreetAddress>16A IVALOO ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>KJXAVAZD21F4</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>VIRTUAL COLLABORATION RESEARCH INC</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Virtual Collaboration Research Inc.]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021411057</ZipCode>
<StreetAddress><![CDATA[1035 Cambridge Street, Geek Offi]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5373</Code>
<Text>SBIR Phase II</Text>
</ProgramElement>
<ProgramReference>
<Code>010E</Code>
<Text>DISABILITY RES &amp; HOMECARE TECH</Text>
</ProgramReference>
<ProgramReference>
<Code>8018</Code>
<Text>Smart and Connected Health</Text>
</ProgramReference>
<ProgramReference>
<Code>8042</Code>
<Text>Health and Safety</Text>
</ProgramReference>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002122DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2021~999998</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-850ae369-7fff-976d-df0e-3859216dfb1b"> </span></p> <p dir="ltr"><span>NavigAid is an AI-driven Orientation and Mobility system designed to empower visually impaired individuals with unprecedented navigational independence using their smartphones. With an innovative neural network architecture called Ally Networks, NavigAid offers users the ability to understand their environment, locate objects, and navigate comfortably in cluttered environments such as subway stations and hospital lobbies. Spanning over multiple phases, this project has aimed to optimize Ally Networks, develop essential NavigAid components, and conduct user experience tests to ensure a positive impact on the 253 million visually impaired people worldwide.</span></p> <p dir="ltr"><span>Throughout the course of the project, our team successfully optimized our core technology, Ally Networks. This neural network architecture allows NavigAid to understand complex environments and provide meaningful, contextual information to users. We also developed two critical NavigAid components: verbal narration of environments and route creation and tracking systems.&nbsp; We achieved this by implementing multimodal training regimes, neural network pruning and quantization to improve network performance, reducing its size by 40% with only a 3% reduction in performance. Our team developed prototype mobile applications for integrating AR services into NavigAid by creating a pipeline to collect rich sensor data from users' smartphones. We also developed a system to align users with saved routes using visual, depth features, and magnetometer readings. As a part of our research, we worked closely with visually impaired users through an intensive co-design process to ensure a seamless user experience. This collaboration allowed us to fine-tune our application and ensure it addresses the unique needs of visually impaired individuals. Our project faced a few minor challenges during data preparation and pilot training phases, with technical issues in cloud computing and neural network training. Additionally, we faced power drainage concerns with NavigAid components due to the use of LIDAR sensors and real-time image processing. However, we addressed these challenges effectively through optimization and data augmentation techniques.</span></p> <p dir="ltr"><span>Between the second and third interim reports, the project underwent a significant pivot. Our research indicated new directions in 3D mapping and localization using neural networks and mobile sensors. Consequently, our focus shifted towards real-time 3D data generation and the development of a cross-platform collaboration tool. This pivot allowed us to design a real-time 3D scanning and streaming service for the real estate market and a web application for cross-platform collaboration. The results have been promising, with accurate 3D model generation and successful development of both back-end and front-end applications Our updated objectives included the development of a 3D reconstruction and streaming service to enable real time collaboration between remote parties. Our real-time reconstruction system based on Ally Networks yielded satisfactory results, producing an accurate reconstruction of indoor spaces within 1cm error margin. Our streaming service can stream 3D models in near-real time speeds (3 seconds/frame), and allows sharing real-time positional information.</span></p> <p dir="ltr"><span>In addition to achieving our technical objectives, we also made significant commercial progress. In parallel to our research, we have developed and deployed our MVP product, Supersense, which quickly became one of the top assistive technology solutions in the market. Supersense has been downloaded over 100,000 times and received recognition as one of the top mobile applications for the visually impaired. Our application has received numerous awards and recognitions including the prestigious Product Hunt App of the Year in the AR category.&nbsp; We increased our sales and gross revenues by 240% when compared to 2020, and our targeted marketing and community engagement strategies allowed us to expand our user base and target audience. We have also made significant progress in our new market.&nbsp; We developed a 3D scanning app, mobile front-end and web-based applications for real-time 3D collaboration.&nbsp;</span>In parallel, we introduced LocSnap, a location sharing platform, which uses our 3D reconstruction and streaming service. We formed a beta group and released our first beta application. We plan to publicly release our application following the beta phase.&nbsp;</p> <p dir="ltr"><span>Our NavigAid project has made significant progress in achieving its original objectives while adapting to a pivot, which has allowed us to explore a broader range of applications and markets. By empowering visually impaired users with a reliable navigation system and developing cutting-edge 3D scanning and streaming technologies, our work can impact and enhance the lives of millions of disadvantaged individuals worldwide.&nbsp;</span></p> <p>&nbsp;</p><br> <p>            Last Modified: 10/19/2023<br>      Modified by: Cagri&nbsp;H&nbsp;Zaman</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   NavigAid is an AI-driven Orientation and Mobility system designed to empower visually impaired individuals with unprecedented navigational independence using their smartphones. With an innovative neural network architecture called Ally Networks, NavigAid offers users the ability to understand their environment, locate objects, and navigate comfortably in cluttered environments such as subway stations and hospital lobbies. Spanning over multiple phases, this project has aimed to optimize Ally Networks, develop essential NavigAid components, and conduct user experience tests to ensure a positive impact on the 253 million visually impaired people worldwide. Throughout the course of the project, our team successfully optimized our core technology, Ally Networks. This neural network architecture allows NavigAid to understand complex environments and provide meaningful, contextual information to users. We also developed two critical NavigAid components: verbal narration of environments and route creation and tracking systems.  We achieved this by implementing multimodal training regimes, neural network pruning and quantization to improve network performance, reducing its size by 40% with only a 3% reduction in performance. Our team developed prototype mobile applications for integrating AR services into NavigAid by creating a pipeline to collect rich sensor data from users' smartphones. We also developed a system to align users with saved routes using visual, depth features, and magnetometer readings. As a part of our research, we worked closely with visually impaired users through an intensive co-design process to ensure a seamless user experience. This collaboration allowed us to fine-tune our application and ensure it addresses the unique needs of visually impaired individuals. Our project faced a few minor challenges during data preparation and pilot training phases, with technical issues in cloud computing and neural network training. Additionally, we faced power drainage concerns with NavigAid components due to the use of LIDAR sensors and real-time image processing. However, we addressed these challenges effectively through optimization and data augmentation techniques. Between the second and third interim reports, the project underwent a significant pivot. Our research indicated new directions in 3D mapping and localization using neural networks and mobile sensors. Consequently, our focus shifted towards real-time 3D data generation and the development of a cross-platform collaboration tool. This pivot allowed us to design a real-time 3D scanning and streaming service for the real estate market and a web application for cross-platform collaboration. The results have been promising, with accurate 3D model generation and successful development of both back-end and front-end applications Our updated objectives included the development of a 3D reconstruction and streaming service to enable real time collaboration between remote parties. Our real-time reconstruction system based on Ally Networks yielded satisfactory results, producing an accurate reconstruction of indoor spaces within 1cm error margin. Our streaming service can stream 3D models in near-real time speeds (3 seconds/frame), and allows sharing real-time positional information. In addition to achieving our technical objectives, we also made significant commercial progress. In parallel to our research, we have developed and deployed our MVP product, Supersense, which quickly became one of the top assistive technology solutions in the market. Supersense has been downloaded over 100,000 times and received recognition as one of the top mobile applications for the visually impaired. Our application has received numerous awards and recognitions including the prestigious Product Hunt App of the Year in the AR category.  We increased our sales and gross revenues by 240% when compared to 2020, and our targeted marketing and community engagement strategies allowed us to expand our user base and target audience. We have also made significant progress in our new market.  We developed a 3D scanning app, mobile front-end and web-based applications for real-time 3D collaboration. In parallel, we introduced LocSnap, a location sharing platform, which uses our 3D reconstruction and streaming service. We formed a beta group and released our first beta application. We plan to publicly release our application following the beta phase.  Our NavigAid project has made significant progress in achieving its original objectives while adapting to a pivot, which has allowed us to explore a broader range of applications and markets. By empowering visually impaired users with a reliable navigation system and developing cutting-edge 3D scanning and streaming technologies, our work can impact and enhance the lives of millions of disadvantaged individuals worldwide.           Last Modified: 10/19/2023       Submitted by: Cagri H Zaman]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
