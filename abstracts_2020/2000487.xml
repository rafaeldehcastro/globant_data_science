<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[Using Neural Networks for Automated Classification of Elementary Mathematics Instructional Activities]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2020</AwardEffectiveDate>
<AwardExpirationDate>08/31/2024</AwardExpirationDate>
<AwardTotalIntnAmount>1499962.00</AwardTotalIntnAmount>
<AwardAmount>1499962</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>11090000</Code>
<Directorate>
<Abbreviation>EDU</Abbreviation>
<LongName>Directorate for STEM Education</LongName>
</Directorate>
<Division>
<Abbreviation>DRL</Abbreviation>
<LongName>Division Of Research On Learning</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Finbarr Sloane</SignBlockName>
<PO_EMAI>fsloane@nsf.gov</PO_EMAI>
<PO_PHON>7032928465</PO_PHON>
</ProgramOfficer>
<AbstractNarration><![CDATA[This research project is supported by the EHR Core Research (ECR) program, which supports work that advances fundamental research on STEM learning and learning environments, broadening participation in STEM, and STEM workforce development.&lt;br/&gt;&lt;br/&gt;In the last decade, there has been a tremendous increase in the use of video for preparing teachers and studying teaching quality. Prominent approaches to summative evaluation of teaching candidates and beginning teachers feature video recordings of instructional practices. In addition, large-scale research studies have included video of instruction. Teacher preparation programs increasingly use video in methods courses and for formative assessment purposes. Moreover, the growing use of interactive simulation in pre-service preparation and in-service professional development relies on having video recorded examples of exemplary instructional practices. Despite the significant growth in the use of video to measure and promote instructional quality in recent years, there remain some key challenges to employing it at scale. First, it is very time-consuming for trained human raters to view hundreds of hours of video recorded lessons. Second, financial costs and time demands increase with the use of multiple human raters per video. Third, manually cataloging, labeling, and indexing large volumes of classroom video for later viewing is time consuming. Recent advances in computer vision, machine learning, and deep learning may provide solutions to these challenges and could make the process of analyzing and scoring videos more efficient. In particular, deep learning has become the state-of-the-art choice in problems related to analyzing the content of video. This proposed NSF Core Research study will draw on videos of elementary mathematics instruction that were collected for two NSF-funded studies that featured the Mathematics-Scan (M-Scan) classroom observation tool. The research will use these videos to explore several ways that deep neural networks can be used to classify instructional activities in videos of math instruction.&lt;br/&gt;&lt;br/&gt;This study will advance knowledge and understanding by examining the degree to which three types of artificial neural networks can accurately classify (a) objects and (b) instructional activities in videos of elementary mathematics instruction. For example, it may be straightforward for neural networks to determine whether an elementary teacher is engaged in lecture vs. facilitating discussion with students. On the other hand, it may be harder for such networks to assess the ways teachers represent math content, the nature of their questions, and whether student gestures signify understanding. The research design concerns three aspects of using computer vision, machine learning, and deep learning: (a) the type of neural network, (b) the type of video label (i.e., object labels and instructional activity labels), and (c) the subject of math instruction (e.g., number and operations; patterns, functions, and algebra; geometry). A few types of neural networks have recently proven effective for video classification: convolutional neural networks (CNNs), long short-term memory (LSTM) neural networks, and hybrid CNN-LSTMs. Ultimately, this project is aimed at beginning to build systematic infrastructure for classifying videos of classroom instruction at scale in efficient and affordable ways. The findings will potentially have key implications for (a) large-scale research studies that feature videos of instruction and (b) pre-service teacher preparation programs, in-service professional development activities, and efforts to evaluate teaching candidates and practicing teachers. In particular, the results from this study will inform decisions about the types of neural networks that can be used to correctly classify videos of instruction and the practical limitations of using networks for this purpose.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.]]></AbstractNarration>
<MinAmdLetterDate>07/21/2020</MinAmdLetterDate>
<MaxAmdLetterDate>06/27/2022</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2000487</AwardID>
<Investigator>
<FirstName>Peter</FirstName>
<LastName>Youngs</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Peter Youngs</PI_FULL_NAME>
<EmailAddress><![CDATA[pay2n@virginia.edu]]></EmailAddress>
<NSF_ID>000088045</NSF_ID>
<StartDate>07/21/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ginger</FirstName>
<LastName>Watson</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ginger S Watson</PI_FULL_NAME>
<EmailAddress><![CDATA[gswatson@odu.edu]]></EmailAddress>
<NSF_ID>000778400</NSF_ID>
<StartDate>07/21/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Virginia Main Campus</Name>
<CityName>CHARLOTTESVILLE</CityName>
<ZipCode>229034833</ZipCode>
<PhoneNumber>4349244270</PhoneNumber>
<StreetAddress>1001 EMMET ST N</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<StateCode>VA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>VA05</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>JJG6HU8PA4S5</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>RECTOR &amp; VISITORS OF THE UNIVERSITY OF VIRGINIA</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[The Rector & Visitors of the University of Virginia]]></Name>
<CityName>Charlottesville</CityName>
<StateCode>VA</StateCode>
<ZipCode>229044195</ZipCode>
<StreetAddress><![CDATA[P.O. Box 400195]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>VA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7980</Code>
<Text>ECR-EDU Core Research</Text>
</ProgramElement>
<ProgramReference>
<Code>8817</Code>
<Text>STEM Learning &amp; Learning Environments</Text>
</ProgramReference>
<Appropriation>
<Code>0420</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0422</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>04002021DB</Code>
<Name><![CDATA[NSF Education & Human Resource]]></Name>
<FUND_SYMB_ID>040106</FUND_SYMB_ID>
</Fund>
<Fund>
<Code>04002223DB</Code>
<Name><![CDATA[NSF Education & Human Resource]]></Name>
<FUND_SYMB_ID>040106</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~1079990</FUND_OBLG>
<FUND_OBLG>2022~419972</FUND_OBLG>
</Award>
</rootTag>
