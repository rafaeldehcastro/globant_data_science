<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[SBIR Phase I:  MuukTest Artificial Intelligence Powered Software Testing]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2020</AwardEffectiveDate>
<AwardExpirationDate>08/31/2022</AwardExpirationDate>
<AwardTotalIntnAmount>256000.00</AwardTotalIntnAmount>
<AwardAmount>256000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>15030000</Code>
<Directorate>
<Abbreviation>TIP</Abbreviation>
<LongName>Dir for Tech, Innovation, &amp; Partnerships</LongName>
</Directorate>
<Division>
<Abbreviation>TI</Abbreviation>
<LongName>Translational Impacts</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Peter Atherton</SignBlockName>
<PO_EMAI>patherto@nsf.gov</PO_EMAI>
<PO_PHON>7032928772</PO_PHON>
</ProgramOfficer>
<AbstractNarration><![CDATA[The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project will enable non-technical users to create complete and comprehensive software test automation. Additionally, it will enable growing software companies to ship their products faster with higher quality at a reasonable cost. Currently, these companies spend up to 50% of their resources on quality assurance (QA) and testing. This project will develop an automated process for testing software.&lt;br/&gt;&lt;br/&gt;This Small Business Innovation Research (SBIR) Phase I project will combine symbolic reasoning algorithms, deep learning, and reinforcement learning models to automate software quality assurance testing. The two problems being addressed by this project are (1) software development teams spend up to 50% of their time testing software, and (2) they have to hire skilled software engineers to automate these tests. The objective of this research is to use artificial intelligence (AI) to make software quality assurance testing faster and enable non-technical workers to create sophisticated tests without the need to code. The proposed research aims to create an AI prototype by (i) building a baseline of manual test scenarios, (ii) building tests using symbolic reasoning (SR), (iii) incorporating deep learning, and (iv) adding reinforcement learning to generate tests.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.]]></AbstractNarration>
<MinAmdLetterDate>09/01/2020</MinAmdLetterDate>
<MaxAmdLetterDate>01/25/2022</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041, 47.084</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2016187</AwardID>
<Investigator>
<FirstName>Ivan</FirstName>
<LastName>Barajas Vargas</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ivan A Barajas Vargas</PI_FULL_NAME>
<EmailAddress><![CDATA[ivan@muuklabs.com]]></EmailAddress>
<NSF_ID>000804454</NSF_ID>
<StartDate>09/01/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>MUUKLABS INC.</Name>
<CityName>RALEIGH</CityName>
<ZipCode>276031568</ZipCode>
<PhoneNumber>9194073193</PhoneNumber>
<StreetAddress>400 W NORTH ST</StreetAddress>
<StreetAddress2><![CDATA[APT 1500]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC13</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>Z8CXPB6WC6Y7</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>MUUKLABS INC</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[MUUKLABS INC.]]></Name>
<CityName/>
<StateCode>NC</StateCode>
<ZipCode>276031568</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5371</Code>
<Text>SBIR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>6856</Code>
<Text>ARTIFICIAL INTELL &amp; COGNIT SCI</Text>
</ProgramReference>
<ProgramReference>
<Code>8032</Code>
<Text>Software Services and Applications</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~256000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-ab7b6ff1-7fff-d4f3-7a7e-d69a105ff978"> <p dir="ltr"><strong>Background on Unmet Need and Technology</strong></p> <p dir="ltr"><span>Software engineers are often compared to civil engineers because they both build large artifacts over long periods of time with hundreds of interacting engineers. However, the similarities end when considering how both groups ensure the correctness of their efforts. Structural and material science, for example, provide a rich theory to predict the tolerances and behavior of bridges and buildings long before they are constructed. In contrast, software systems are rarely well understood even after being deployed. This is partly because software systems have much more varying requirements than structures. Still, it is also because the theory of software systems is lacking compared to that of structural soundness.</span></p> <p dir="ltr"><span>As a result, the software industry employs teams of engineers who focus exclusively on testing their software for defects because they cannot know beforehand whether the software will perform correctly</span><span>. These software quality assurance (SQA) teams consume a large percentage of the software development budget. They must acquire expert craftsmanship in devising testing suites over years of practice and training. Most software organizations do not consider this a strange situation and simply accept it. </span><span>The proposed solution, MuukTest AI, aims to bring this level of clarity to software quality assurance (SQA)</span><span> by performing two major goals:</span></p> <p dir="ltr"><em>MuukTest AI simulates users. </em><span>MuukTest AI simulates users by exploring Web applications using tools that communicate with Web browsers. Modern applications deployed as Web or Mobile define their elements into an execution tree that is easily accessible to read or update. The proposed agent will </span><span>find</span><span> and </span><span>exercise</span><span> elements available under the HTML Document Object Model (DOM) tree. The smart agent will work efficiently by leveraging different AI/ML approaches, including Reinforcement Learning.</span></p> <p dir="ltr"><em>MuukTest AI will identify unexpected behaviors in web applications. </em><span>The main goal of software testing is the identification of defects and unexpected results; in modern Web applications, some unexpected behaviors may be obvious, e.g., when the Web application literally crashes and stops responding, this unexpected behavior could be easily recognized since the following steps could not be completed. Additionally, software defects are triggered by a deep understanding of the software business logic. Subsequently, MuukTest AI will need to comprehend the software business logic of the Web applications under test to identify defects and unexpected behaviors correctly.&nbsp;&nbsp;&nbsp;</span></p> <p dir="ltr"><strong>Summary of Research Performed and Results</strong></p> <br /> <p dir="ltr"><span>Based on the Phase I proposal, the overall goal was to implement a prototype that performs the user simulation exercising AI/ML models. The MuukLabs research team has successfully built such a prototype, proving the feasibility of combining the AI models and a web explorer. The learning process exercises an algorithm for each state observed and receives a reward. The knowledge gets tracked and continues being updated during the learning iterations. </span><span>Such a process is an innovative solution using open source tools that had not been implemented for Software Quality Assurance procedures, only in gaming and other use cases</span><span>.&nbsp;</span></p> <br /><br /> <p dir="ltr"><span>The experiments performed were divided into two sets: self-learning explorations and pre-trained explorations. The self-learning explorations refer to the agents building their knowledge with no previous context. On the other hand, the trained explorations experiment received a pre-trained set of data from the self-learning exploration. The goal was to evaluate if the test cases created during the self-learning exploration cycle could be used as valid and how the pre-trained data improved the explorations and new test cases. In this context, a valid test case is a repeatable discovery that can be executed in a web application and could potentially find defects. The results show that during the training cycles with no pre-trained data, the agents can create valid test cases 23% of the time. Furthermore, using trained data, this number increased to 45%.&nbsp;</span></p> <br /> <p dir="ltr"><span>In summary, the implemented architecture enables MuukTest AI to run a learning process, creating a cycle where the agents request to apply a specific action based on the state. The Phase I research proved that the architecture can be upgraded from a prototype to a commercial version, providing value to new customers looking to improve software quality in their products.</span></p> <br /> <p dir="ltr"><em>Explorer and Agents communicating with MuukTest codeless platform.</em><span> </span><span>In addition to this investigation&rsquo;s results, we have been able to communicate the Explorer and Agent modules with the MuukTest codeless platform. The prototype can automatically create test cases for each exploration performed. At this stage, a QA MuukTest engineer curates the information, which defines whether it is a duplicated exploration or an actual defect. Eventually, this work will be automated by the AI agents.&nbsp;</span></p> <div></div> </span></p><br> <p>            Last Modified: 10/03/2022<br>      Modified by: Ivan&nbsp;A&nbsp;Barajas Vargas</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Background on Unmet Need and Technology Software engineers are often compared to civil engineers because they both build large artifacts over long periods of time with hundreds of interacting engineers. However, the similarities end when considering how both groups ensure the correctness of their efforts. Structural and material science, for example, provide a rich theory to predict the tolerances and behavior of bridges and buildings long before they are constructed. In contrast, software systems are rarely well understood even after being deployed. This is partly because software systems have much more varying requirements than structures. Still, it is also because the theory of software systems is lacking compared to that of structural soundness. As a result, the software industry employs teams of engineers who focus exclusively on testing their software for defects because they cannot know beforehand whether the software will perform correctly. These software quality assurance (SQA) teams consume a large percentage of the software development budget. They must acquire expert craftsmanship in devising testing suites over years of practice and training. Most software organizations do not consider this a strange situation and simply accept it. The proposed solution, MuukTest AI, aims to bring this level of clarity to software quality assurance (SQA) by performing two major goals: MuukTest AI simulates users. MuukTest AI simulates users by exploring Web applications using tools that communicate with Web browsers. Modern applications deployed as Web or Mobile define their elements into an execution tree that is easily accessible to read or update. The proposed agent will find and exercise elements available under the HTML Document Object Model (DOM) tree. The smart agent will work efficiently by leveraging different AI/ML approaches, including Reinforcement Learning. MuukTest AI will identify unexpected behaviors in web applications. The main goal of software testing is the identification of defects and unexpected results; in modern Web applications, some unexpected behaviors may be obvious, e.g., when the Web application literally crashes and stops responding, this unexpected behavior could be easily recognized since the following steps could not be completed. Additionally, software defects are triggered by a deep understanding of the software business logic. Subsequently, MuukTest AI will need to comprehend the software business logic of the Web applications under test to identify defects and unexpected behaviors correctly.    Summary of Research Performed and Results   Based on the Phase I proposal, the overall goal was to implement a prototype that performs the user simulation exercising AI/ML models. The MuukLabs research team has successfully built such a prototype, proving the feasibility of combining the AI models and a web explorer. The learning process exercises an algorithm for each state observed and receives a reward. The knowledge gets tracked and continues being updated during the learning iterations. Such a process is an innovative solution using open source tools that had not been implemented for Software Quality Assurance procedures, only in gaming and other use cases.     The experiments performed were divided into two sets: self-learning explorations and pre-trained explorations. The self-learning explorations refer to the agents building their knowledge with no previous context. On the other hand, the trained explorations experiment received a pre-trained set of data from the self-learning exploration. The goal was to evaluate if the test cases created during the self-learning exploration cycle could be used as valid and how the pre-trained data improved the explorations and new test cases. In this context, a valid test case is a repeatable discovery that can be executed in a web application and could potentially find defects. The results show that during the training cycles with no pre-trained data, the agents can create valid test cases 23% of the time. Furthermore, using trained data, this number increased to 45%.    In summary, the implemented architecture enables MuukTest AI to run a learning process, creating a cycle where the agents request to apply a specific action based on the state. The Phase I research proved that the architecture can be upgraded from a prototype to a commercial version, providing value to new customers looking to improve software quality in their products.   Explorer and Agents communicating with MuukTest codeless platform. In addition to this investigationâ€™s results, we have been able to communicate the Explorer and Agent modules with the MuukTest codeless platform. The prototype can automatically create test cases for each exploration performed. At this stage, a QA MuukTest engineer curates the information, which defines whether it is a duplicated exploration or an actual defect. Eventually, this work will be automated by the AI agents.          Last Modified: 10/03/2022       Submitted by: Ivan A Barajas Vargas]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
