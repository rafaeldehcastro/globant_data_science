<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[EAGER: Minimal 3D Modeling Methodology]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2020</AwardEffectiveDate>
<AwardExpirationDate>06/30/2022</AwardExpirationDate>
<AwardTotalIntnAmount>65000.00</AwardTotalIntnAmount>
<AwardAmount>65000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032924341</PO_PHON>
</ProgramOfficer>
<AbstractNarration><![CDATA[Modeling and design are core components of computer graphics and computer vision research and applications.  Traditional modeling consists of having the designer provide either a detailed (digital) specification of the desired virtual object or sufficient photographs of a physical object to enable a multi-view stereo reconstruction, although modern GUI-based tools can help reduce to a certain extent the number of photographs required.  Digital sketching tools provide an alternative mechanism for modeling objects, but even though some of these try to assist the user by completing partial sketches a notable effort is still required to achieve detailed results.  This project will explore a modeling methodology that addresses the following question: What is the least we can design and still obtain a sufficiently expressive system?  At one extreme digital modeling tools support high expressivity but require high design effort, while at the other extreme providing a fixed set of model templates incurs very low design effort but results in low model expressivity as well.  Some recent efforts, such as the PI's sketch-to-procedural-modeling work, fall somewhere in the middle.  The goal of the current research is to determine the point of optimum balance between design and expressivity, that is just enough design effort to produce a sufficiently expressive model. The focus of this multi-disciplinary work will be on computational archaeology, an interesting application where only fragmented information is available, hence success of the approach in this domain will imply broad generalizability of project outcomes to other areas as well.&lt;br/&gt;&lt;br/&gt;It has been established in the literature that only a fraction of what we perceive suffices for a person to create a mental 3D representation of an object.  With this observation in mind, the project will build upon and extend the PI's existing photograph-to-3D modeling tool by adding new minimalist machine learning underpinnings applied to urban and archaeological modeling and design, to build software that requests just enough input from the user and is able to produce 3D models of sufficient completeness for the intended goal.  Various ways of degrading the input detail and analyzing how the model output is affected will be explored to identify the most promising for retention and improvement of robustness.  About a terabyte of imagery and point cloud data from two archaeological sites of ancient settlements on the islands of Dana and Bogsak along the southern coast of Turkey will serve as a testbed.  These islands have structures built using material from local stone quarries and are a main type of urban landscape to survive from antiquity but are difficult to study due to size, complexity, terrain, and incompleteness.  Nonetheless, aerial drone-based imagery and LIDAR are possible.  The research will investigate how much must be specified during design to differentiate among the possible forms and their parameters to express a desired output.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.]]></AbstractNarration>
<MinAmdLetterDate>06/11/2020</MinAmdLetterDate>
<MaxAmdLetterDate>06/11/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2032770</AwardID>
<Investigator>
<FirstName>Daniel</FirstName>
<LastName>Aliaga</LastName>
<PI_MID_INIT>G</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Daniel G Aliaga</PI_FULL_NAME>
<EmailAddress><![CDATA[aliaga@cs.purdue.edu]]></EmailAddress>
<NSF_ID>000483177</NSF_ID>
<StartDate>06/11/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Nicholas</FirstName>
<LastName>Rauh</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Nicholas K Rauh</PI_FULL_NAME>
<EmailAddress><![CDATA[rauhn@purdue.edu]]></EmailAddress>
<NSF_ID>000086602</NSF_ID>
<StartDate>06/11/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Purdue University</Name>
<CityName>WEST LAFAYETTE</CityName>
<ZipCode>479061332</ZipCode>
<PhoneNumber>7654941055</PhoneNumber>
<StreetAddress>2550 NORTHWESTERN AVE STE 1900</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN04</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>YRXVL4JYCEF5</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>PURDUE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM>YRXVL4JYCEF5</ORG_PRNT_UEI_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Purdue University]]></Name>
<CityName/>
<StateCode>IN</StateCode>
<ZipCode>479062183</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~65000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>This project pursues a modeling methodology that addresses the following question: "what is the least we can design and still obtain a sufficiently expressive system?" One important consequence of solving this question is that 2D and 3D structures can be modeled from only fragmented information. Rather than assuming all details are sampled, most of the observed object is inferred. </span></p> <p><span>This AI-based approach is of particular importance to urban modeling of current structures and of past structures (i.e., archaeology). As such, this project has stimulated the creation of an international collaboration of archaeologists and computer scientists in the area of non-invasive archaeology. This recent trend attempts to discover the past without having to dig-out (e.g., uncover) historical sites which is expensive, dangerous, and potentially harming to the structures themselves. Initial results of this method have been applied to actual archaeological sites and data for other sites has been collected and is being processed. </span></p> <p><span>In addition, a derivative of this work is to develop tools to infer detailed 3D models of existing cities using only a small fragment of information. Rather than assuming a dense capture of a complex city, a significant amount is generated (or "hallucinated") so that it matches what is observed and also exploits general rules of how urban structures (or urban vegetation) is organized. This significantly accelerates and simplifies the modeling process. While the results might not be photographically identical, they are functionally very similar and thus can be used for a wide variety of planning, what-if scenario, urban forestry services, and even urban meteorological planning.</span></p> <p><span>This EAGER grant has already generated several publications and subsequent NSF and USDA grants to further explore this direction.</span></p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/31/2022<br>      Modified by: Daniel&nbsp;G&nbsp;Aliaga</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2022/2032770/2032770_10676461_1667253088218_Capture--rgov-214x142.jpg" original="/por/images/Reports/POR/2022/2032770/2032770_10676461_1667253088218_Capture--rgov-800width.jpg" title="Deep Non-Invasive Deep Archaeological"><img src="/por/images/Reports/POR/2022/2032770/2032770_10676461_1667253088218_Capture--rgov-66x44.jpg" alt="Deep Non-Invasive Deep Archaeological"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Building Contour Completion. (i) Our approach supports pluralistic completions. (ii) User is ableto complete highly incomplete building contours in just a few iterations. (iii) We demonstrate on real-worldarchaeological sites.</div> <div class="imageCredit">Daniel Aliaga</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Daniel&nbsp;G&nbsp;Aliaga</div> <div class="imageTitle">Deep Non-Invasive Deep Archaeological</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project pursues a modeling methodology that addresses the following question: "what is the least we can design and still obtain a sufficiently expressive system?" One important consequence of solving this question is that 2D and 3D structures can be modeled from only fragmented information. Rather than assuming all details are sampled, most of the observed object is inferred.   This AI-based approach is of particular importance to urban modeling of current structures and of past structures (i.e., archaeology). As such, this project has stimulated the creation of an international collaboration of archaeologists and computer scientists in the area of non-invasive archaeology. This recent trend attempts to discover the past without having to dig-out (e.g., uncover) historical sites which is expensive, dangerous, and potentially harming to the structures themselves. Initial results of this method have been applied to actual archaeological sites and data for other sites has been collected and is being processed.   In addition, a derivative of this work is to develop tools to infer detailed 3D models of existing cities using only a small fragment of information. Rather than assuming a dense capture of a complex city, a significant amount is generated (or "hallucinated") so that it matches what is observed and also exploits general rules of how urban structures (or urban vegetation) is organized. This significantly accelerates and simplifies the modeling process. While the results might not be photographically identical, they are functionally very similar and thus can be used for a wide variety of planning, what-if scenario, urban forestry services, and even urban meteorological planning.  This EAGER grant has already generated several publications and subsequent NSF and USDA grants to further explore this direction.             Last Modified: 10/31/2022       Submitted by: Daniel G Aliaga]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
