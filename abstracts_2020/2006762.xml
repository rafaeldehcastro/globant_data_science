<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[Collaborative Research: CIF: Small: Convexification-based Decomposition Methods for Large-Scale Inference in Graphical Models]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2020</AwardEffectiveDate>
<AwardExpirationDate>06/30/2023</AwardExpirationDate>
<AwardTotalIntnAmount>250002.00</AwardTotalIntnAmount>
<AwardAmount>250002</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>James Fowler</SignBlockName>
<PO_EMAI>jafowler@nsf.gov</PO_EMAI>
<PO_PHON>7032928910</PO_PHON>
</ProgramOfficer>
<AbstractNarration><![CDATA[Systems prevalent in modern society can be characterized by complex networks of interconnected components that generate massive amounts of data. The ability to make timely inferences using these data presents unprecedented opportunities to solve major societal problems. For example, advances in wearable technology are transforming the delivery of personalized healthcare and wellness programs. More broadly, wearables naturally create sensor networks over populations and the data from these networks can be harnessed to detect and/or prevent diseases, crimes or environmental hazards. Inference from such data can be naturally accomplished using graphical models. Unfortunately, existing technology for graphical models requires stringent assumptions that are seldom satisfied in modern applications. The goal of this project is to address these shortcomings by developing new computational methods that automatically infer the topology of a graphical model from high-dimensional data, identify and/or correct outliers and anomalies, and solve the estimation problems simultaneously. Furthermore, the proposed research will lead to innovative teaching material defining modern data science curricula and develop a diverse cadre of Ph.D. students with skills at the interface of discrete optimization, continuous optimization, and statistics.&lt;br/&gt;&lt;br/&gt;Inference problems with spurious data and unknown network topologies can be modeled as large-scale constrained mixed-integer convex optimization problems. To address the challenges posed by the presence of the combinatorial constraints, this project employs a combination of two key ideas. The first idea is to decompose the problem into progressively small problems, that can be solved in a decentralized and parallel fashion, by leveraging the Markov property inherent in graphical models. The second idea is the convexification of the combinatorial constraints, to diminish or prevent altogether the loss in quality from the decomposition of the problem. Unlike typical decomposition methods such as Lagrangian relaxation, which can lead to large duality gaps, this project will develop novel techniques based on convexification and Fenchel duality. In particular, the resulting method will account for the combinatorial restrictions and the nonlinear loss function concurrently, ultimately resulting in small or no duality gaps. The successful completion of the project will lead to significant advances in inference with spatio-temporal data, interpretable prediction, and identification of causal relationships.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.]]></AbstractNarration>
<MinAmdLetterDate>06/22/2020</MinAmdLetterDate>
<MaxAmdLetterDate>06/22/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2006762</AwardID>
<Investigator>
<FirstName>Andres</FirstName>
<LastName>Gomez</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Andres Gomez</PI_FULL_NAME>
<EmailAddress><![CDATA[gomezand@usc.edu]]></EmailAddress>
<NSF_ID>000755902</NSF_ID>
<StartDate>06/22/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Southern California</Name>
<CityName>LOS ANGELES</CityName>
<ZipCode>900894304</ZipCode>
<PhoneNumber>2137407762</PhoneNumber>
<StreetAddress>3720 S FLOWER ST</StreetAddress>
<StreetAddress2><![CDATA[FL 3]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA37</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>G88KLJR3KYT5</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF SOUTHERN CALIFORNIA</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Southern California]]></Name>
<CityName>Los Angeles</CityName>
<StateCode>CA</StateCode>
<ZipCode>900890001</ZipCode>
<StreetAddress><![CDATA[3650 McClintock Avenue OHE 310 B]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA37</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7797</Code>
<Text>Comm &amp; Information Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7936</Code>
<Text>SIGNAL PROCESSING</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~250002</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Prevalent systems in contemporary society can be characterized by complex networks of interconnected components that generate massive amounts of data (e.g., wireless networks of sensors or wearables, and gene regulatory networks). The ability to make timely inferences using these data presents unprecedented opportunities to tackle major societal problems, ranging from managing population health to understanding complex diseases. &nbsp;Graphical models are a natural fit to use in the context of inference problems over networks. However, existing graphical models and methods have inherent limitations that preclude their use in modern applications: the exact methods do not scale to high-dimensional data, they often assume that the structure of the graphical model is known a priori and that the data do not contain outliers or anomalies. Due to scalability concerns associated with solving large-scale combinatorial inference problems to optimality, heuristic methods are often used in practice. Unfortunately, the statistical properties of the solutions of such heuristics are far from clear, resulting in a gap between theory and computation. In this project, we close we close this knowledge gap. The proposed work differs from the status quo in that we develop exact and scalable computational methods that automatically infer the topology of a graphical model from high-dimensional data, identify and/or correct anomalies and solve the estimation problems simultaneously. To do so, we propose to model the inference problems with graphical models as constrained mixed-integer convex optimization problems.</p> <p>&nbsp;</p> <p>The contributions of this project are two-fold. First, we identify a broad class of learning problems with graphical models that, contrary to usual expectations, can in fact be solved efficiently to optimality. The class of efficiently-solvable problems relies on exploiting critical structural properties of graphical models that arise with time series data and Besag-York models (commonly used to model spread of epidemics), and the algorithms extend to problems with the presence of difficult combinatorial constraints such as outliers or interpretability. Naturally, in this case, the exact solution of the problems provides improved statistical performance without additional computational costs. Second, for problems that are not polynomial-time solvable, we propose decomposition methods based on a divide a conquer approach. At a high level, the proposed approach breaks a complex learning problems into simpler structures which can be tackled efficiently, and then combines the solutions obtained by applying the proposed algorithms to the simpler structures. We show that the proposed method outperforms usual existing statistical techniques, and is considerably faster than exact methods relying on off-the-shelf mixed-integer optimization solvers.</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/29/2023<br>      Modified by: Andres&nbsp;Gomez</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Prevalent systems in contemporary society can be characterized by complex networks of interconnected components that generate massive amounts of data (e.g., wireless networks of sensors or wearables, and gene regulatory networks). The ability to make timely inferences using these data presents unprecedented opportunities to tackle major societal problems, ranging from managing population health to understanding complex diseases.  Graphical models are a natural fit to use in the context of inference problems over networks. However, existing graphical models and methods have inherent limitations that preclude their use in modern applications: the exact methods do not scale to high-dimensional data, they often assume that the structure of the graphical model is known a priori and that the data do not contain outliers or anomalies. Due to scalability concerns associated with solving large-scale combinatorial inference problems to optimality, heuristic methods are often used in practice. Unfortunately, the statistical properties of the solutions of such heuristics are far from clear, resulting in a gap between theory and computation. In this project, we close we close this knowledge gap. The proposed work differs from the status quo in that we develop exact and scalable computational methods that automatically infer the topology of a graphical model from high-dimensional data, identify and/or correct anomalies and solve the estimation problems simultaneously. To do so, we propose to model the inference problems with graphical models as constrained mixed-integer convex optimization problems.     The contributions of this project are two-fold. First, we identify a broad class of learning problems with graphical models that, contrary to usual expectations, can in fact be solved efficiently to optimality. The class of efficiently-solvable problems relies on exploiting critical structural properties of graphical models that arise with time series data and Besag-York models (commonly used to model spread of epidemics), and the algorithms extend to problems with the presence of difficult combinatorial constraints such as outliers or interpretability. Naturally, in this case, the exact solution of the problems provides improved statistical performance without additional computational costs. Second, for problems that are not polynomial-time solvable, we propose decomposition methods based on a divide a conquer approach. At a high level, the proposed approach breaks a complex learning problems into simpler structures which can be tackled efficiently, and then combines the solutions obtained by applying the proposed algorithms to the simpler structures. We show that the proposed method outperforms usual existing statistical techniques, and is considerably faster than exact methods relying on off-the-shelf mixed-integer optimization solvers.          Last Modified: 10/29/2023       Submitted by: Andres Gomez]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
