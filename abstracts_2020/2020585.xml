<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[Understanding Deep Neural Networks]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2020</AwardEffectiveDate>
<AwardExpirationDate>08/31/2023</AwardExpirationDate>
<AwardTotalIntnAmount>139360.00</AwardTotalIntnAmount>
<AwardAmount>139360</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Frederick Kronz</SignBlockName>
<PO_EMAI>fkronz@nsf.gov</PO_EMAI>
<PO_PHON>7032927283</PO_PHON>
</ProgramOfficer>
<AbstractNarration><![CDATA[This is a research project on artificial intelligence (AI) that focuses on the problem of explainability. More specifically, it will relate deep learning neural networks to broader explanatory frameworks in cognitive science. The PI plans to develop a philosophically and empirically grounded account of the rational faculties that can be modeled by diverse deep learning neural network architectures. A main goal of this research is to articulate a subtler view of deep learning and the diversity of its architectures and uses, which would be made vibrant and accessible to academic scholars and the broader public via frequent comparisons to research on human and animal cognition. Diverse scholars will benefit from the account of the rational capacities of deep neural networks to be developed in this research. The PI will present findings at conferences and workshops in computational neuroscience, psychology, animal cognition, and philosophy; the results are to be published in a monograph suitable for teaching and research in any of these disciplines. The work will also be presented at conferences for policymakers working on emerging technologies with the aim of helping audiences to peek inside the "black box" of deep learning systems and better anticipate the space of possible policy interventions and their effects. It will also be presented in public fora, such as the Brains Blog, the New York Times’ The Stone column, 3 AM, and philosophy podcasts such as Philosophy Bites.&lt;br/&gt;&lt;br/&gt;This research project is robustly interdisciplinary; the PI will bring together cutting-edge research from computer science, psychology, neuroscience, and philosophy to provide a better understanding of deep learning neural networks. He will begin by analyzing a basic deep learning network template; he will describe the computational problems that plague this template when applied to biologically-relevant problems, and then explore how those limitations can be overcome by adding further biologically-inspired components. These modifications—all of which have already demonstrated their promise in successful models—correspond to biological faculties like reinforcement learning, predictive learning, imagination, attention, episodic memory, executive control, and social cognition. With each component, comparisons will be made to the structure and function the corresponding faculty in animals and humans, the degree of similarity between the model component’s structure and its implementation in biological brains, and the kind or degree of rationality that an architecture deploying those components can exhibit in its decision-making. This strategy allows the exploration of diverse deep learning model architectures (many of which have been little discussed) under a common narrative thread. The results of this project will enable scientists, engineers, and the broader public to understand both the strengths and limits of different deep architectures, and how their capacities relate to those of biological organisms. It is imperative to meet this objective as deep learning continues to be deployed for an increasingly wider range of tasks including image search, facial recognition, driverless automobile navigation, game-playing, medical diagnosis, and many others.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.]]></AbstractNarration>
<MinAmdLetterDate>07/09/2020</MinAmdLetterDate>
<MaxAmdLetterDate>07/09/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2020585</AwardID>
<Investigator>
<FirstName>Cameron</FirstName>
<LastName>Buckner</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Cameron J Buckner</PI_FULL_NAME>
<EmailAddress><![CDATA[cjbuckner@uh.edu]]></EmailAddress>
<NSF_ID>000797111</NSF_ID>
<StartDate>07/09/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Houston</Name>
<CityName>HOUSTON</CityName>
<ZipCode>772043067</ZipCode>
<PhoneNumber>7137435773</PhoneNumber>
<StreetAddress>4300 MARTIN LUTHER KING BLVD</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX18</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>QKWEF8XLMTT3</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF HOUSTON SYSTEM</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Houston]]></Name>
<CityName>Houston</CityName>
<StateCode>TX</StateCode>
<ZipCode>772042015</ZipCode>
<StreetAddress><![CDATA[4800 Calhoun Boulevard]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>124Y</Code>
<Text>Science &amp; Technology Studies</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>8089</Code>
<Text>Understanding the Brain/Cognitive Scienc</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002021DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2020~139360</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Cameron J. Buckner completed the planned research, supported by an NSF Scholar's Award, that developed a systematic philosophy for understanding recent advances in deep learning research, based in empiricist philosophy of mind.</p> <p>During his funding period, Buckner participated in a number of conferences involving philosophers, computer scientists, neuroscientists, psychologists, and computer scientists. He developed and defended a moderate version of empiricism and distinguished it from other, less plausible forms, and argued that the moderate strain is the one that characterizes both the work of historical empiricists like Locke, Hume, James, and Sophie de Grouchy, and contemporary deep learning research. He studied how the functions performed by many cutting-edge deep neural network architectures?including deep convolutional neural networks, generative adversarial networks, variational autoencoders, and transformers?realize some of the historical empiricists? most ambitious ideas. The debate between empiricism and nativism, then and now, concerns the origins of abstract knowledge?is abstract knowledge derived from sensory experience, or is innate, part of our mind?s initial start-up software? This older philosophical debate has important consequences for engineering methodology?for if the nativists are right about the human mind, we might want to program our artificial agents with significant amounts of innate knowledge, too, if we expect them to reach human-like levels of intelligence. However, this historical debate has not been applied to contemporary engineering questions in the most useful way.</p> <p>In particular, most contemporary construals of empiricism invoked by critics of deep learning suppose that when empiricists say that the mind is a ?blank slate?, they mean it starts with almost no innate structure whatsoever?perhaps only one or two rules for associative learning, and anything else must be derived from experience. By contrast, most historical empiricists were faculty psychologists?that is, they believed that the mind begins with a variety of domain-general psychological faculties like perception, memory, imagination, and attention, and it is through interactions amongst these domain-general faculties that the mind is able to extract the most abstract forms of knowledge from experience. The research funded by Buckner explored how many connections could be drawn by empiricist faculty psychology to recent advances in deep learning, which often implement algorithmic innovations which are explicitly inspired by and compared to the functions of these domain general faculties. Aligning classical empiricist faculty psychology with recent deep learning research should bring benefits to both disciplines: computer scientists can continue to mine the history of philosophy for ideas and aspirational targets to hit, and philosophers can see how some of the historical empiricists' most ambitious speculations can now be realized in specific computational systems. To general audiences, the alignment should help non-experts see how the otherwise highly technical operations of deep learning systems behind headline-grabbing achievements can be related back to particular aspects of their own intelligence and rationality, helping to both illuminate prior achievements and temper expectations for future breakthroughs.</p> <p>During the grant period, Buckner delivered lectures on this research to more than 30 invited speaking engagements and peer-reviewed conference presentations, to audiences from a variety of different disciplinary backgrounds, many of which were recorded and remain viewable for free by the general public. The research culminated in a 488 page monograph that will be published in October 2023 with Oxford University Press, entitled <em>From Deep Learning to Rational Machines: What the History of Philosophy Can Teach Us about the Future of Artificial Intelligence</em>, which describes the research outcomes to an interdisciplinary audience.</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/30/2023<br>      Modified by: Cameron&nbsp;J&nbsp;Buckner</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Cameron J. Buckner completed the planned research, supported by an NSF Scholar's Award, that developed a systematic philosophy for understanding recent advances in deep learning research, based in empiricist philosophy of mind.  During his funding period, Buckner participated in a number of conferences involving philosophers, computer scientists, neuroscientists, psychologists, and computer scientists. He developed and defended a moderate version of empiricism and distinguished it from other, less plausible forms, and argued that the moderate strain is the one that characterizes both the work of historical empiricists like Locke, Hume, James, and Sophie de Grouchy, and contemporary deep learning research. He studied how the functions performed by many cutting-edge deep neural network architectures?including deep convolutional neural networks, generative adversarial networks, variational autoencoders, and transformers?realize some of the historical empiricists? most ambitious ideas. The debate between empiricism and nativism, then and now, concerns the origins of abstract knowledge?is abstract knowledge derived from sensory experience, or is innate, part of our mind?s initial start-up software? This older philosophical debate has important consequences for engineering methodology?for if the nativists are right about the human mind, we might want to program our artificial agents with significant amounts of innate knowledge, too, if we expect them to reach human-like levels of intelligence. However, this historical debate has not been applied to contemporary engineering questions in the most useful way.  In particular, most contemporary construals of empiricism invoked by critics of deep learning suppose that when empiricists say that the mind is a ?blank slate?, they mean it starts with almost no innate structure whatsoever?perhaps only one or two rules for associative learning, and anything else must be derived from experience. By contrast, most historical empiricists were faculty psychologists?that is, they believed that the mind begins with a variety of domain-general psychological faculties like perception, memory, imagination, and attention, and it is through interactions amongst these domain-general faculties that the mind is able to extract the most abstract forms of knowledge from experience. The research funded by Buckner explored how many connections could be drawn by empiricist faculty psychology to recent advances in deep learning, which often implement algorithmic innovations which are explicitly inspired by and compared to the functions of these domain general faculties. Aligning classical empiricist faculty psychology with recent deep learning research should bring benefits to both disciplines: computer scientists can continue to mine the history of philosophy for ideas and aspirational targets to hit, and philosophers can see how some of the historical empiricists' most ambitious speculations can now be realized in specific computational systems. To general audiences, the alignment should help non-experts see how the otherwise highly technical operations of deep learning systems behind headline-grabbing achievements can be related back to particular aspects of their own intelligence and rationality, helping to both illuminate prior achievements and temper expectations for future breakthroughs.  During the grant period, Buckner delivered lectures on this research to more than 30 invited speaking engagements and peer-reviewed conference presentations, to audiences from a variety of different disciplinary backgrounds, many of which were recorded and remain viewable for free by the general public. The research culminated in a 488 page monograph that will be published in October 2023 with Oxford University Press, entitled From Deep Learning to Rational Machines: What the History of Philosophy Can Teach Us about the Future of Artificial Intelligence, which describes the research outcomes to an interdisciplinary audience.          Last Modified: 10/30/2023       Submitted by: Cameron J Buckner]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
